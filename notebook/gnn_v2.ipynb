{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_dataset(self):    \n",
    "        \n",
    "        '''\n",
    "        citations: [target논문인덱스, source논문인덱스]\n",
    "        papers: [논문인덱스, 1424개 단어 포함 여부, 주제(subject)]\n",
    "        train_data: papers 데이터 중 50% 샘플링\n",
    "        test_data: papers 데이터 중 50% 샘플링\n",
    "        x_train: train_data 중, 논문인덱스와 subject를 제외한 피쳐\n",
    "        y_train: train_data 중 subject에 해당하는 레이블\n",
    "        '''\n",
    "        \n",
    "        zip_file = keras.utils.get_file(\n",
    "            fname=\"cora.tgz\",\n",
    "            origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "            extract=True,\n",
    "        )\n",
    "        data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "        \n",
    "        citations = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.cites\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"target\", \"source\"],\n",
    "        )\n",
    "        \n",
    "        column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "        papers = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.content\"), \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=column_names,\n",
    "        )\n",
    "        \n",
    "        class_values = sorted(papers[\"subject\"].unique())\n",
    "        class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "        paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "        papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "        papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "        \n",
    "        train_data, test_data = [], []\n",
    "\n",
    "        for _, group_data in papers.groupby(\"subject\"):\n",
    "            # Select around 50% of the dataset for training.\n",
    "            random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "            train_data.append(group_data[random_selection])\n",
    "            test_data.append(group_data[~random_selection])\n",
    "\n",
    "        train_data = pd.concat(train_data).sample(frac=1)\n",
    "        test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "        print(\"citations data shape:\", citations.shape)\n",
    "        print(\"papers data shape:\", papers.shape)\n",
    "        print(\"Train data shape:\", train_data.shape)\n",
    "        print(\"Test data shape:\", test_data.shape)\n",
    "        \n",
    "        feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
    "        num_features = len(feature_names)\n",
    "        num_classes = len(class_idx)\n",
    "\n",
    "        # Create train and test features as a numpy array.\n",
    "        x_train = train_data[feature_names].to_numpy()\n",
    "        x_test = test_data[feature_names].to_numpy()\n",
    "        \n",
    "        # Create train and test targets as a numpy array.\n",
    "        y_train = train_data[\"subject\"]\n",
    "        y_test = test_data[\"subject\"]\n",
    "        \n",
    "        # Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
    "        edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "        # Create an edge weights array of ones.\n",
    "        edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Create a node features array of shape [num_nodes, num_features].\n",
    "        node_features = tf.cast(\n",
    "            papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    "        )\n",
    "        # Create graph info tuple with node_features, edges, and edge_weights.\n",
    "        graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "        print(\"Edges shape:\", edges.shape)\n",
    "        print(\"Nodes shape:\", node_features.shape)\n",
    "        \n",
    "        self.feature_names = feature_names #단어 피쳐 컬럼 이름\n",
    "        self.num_features = num_features #단어 피쳐 개수\n",
    "        self.num_classes = num_classes #주제subject의 가짓수\n",
    "        self.graph_info = graph_info\n",
    "        self.node_features = node_features #논문 인덱스 순으로 정리된 상태의 feature 매트릭스\n",
    "        self.edges = edges\n",
    "        self.class_values = class_values\n",
    "        \n",
    "        return citations, papers, x_train, x_test, y_train, y_test, train_data, test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citations data shape: (5429, 2)\n",
      "papers data shape: (2708, 1435)\n",
      "Train data shape: (1373, 1435)\n",
      "Test data shape: (1335, 1435)\n",
      "Edges shape: (2, 5429)\n",
      "Nodes shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "loader = Loader()\n",
    "citations, papers, x_train, x_test, y_train, y_test, train_data, test_data = loader.load_dataset()\n",
    "feature_names = loader.feature_names\n",
    "num_classes = loader.num_classes\n",
    "graph_info = loader.graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    '''\n",
    "hidden_units = [32, 32]\n",
    "dropout_rate = 0.2\n",
    "'''\n",
    "\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(layers.Layer):\n",
    "    '''\n",
    "    1.gather: 이웃논문별 특성을 모음\n",
    "    2.prepare: 이웃논문별 특성을 ffn한 후, edge점수를 곱함\n",
    "    3.aggregate: 전체논문별로 이웃논문들의 결과를 aggregate함\n",
    "    4.update1: 전체논문별 특성과 위 결과를 concat\n",
    "    5.update2: concat한 것을 ffn해서 최종 embedding도출\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel:\n",
    "    def __init__(self,\n",
    "                num_classes,\n",
    "                hidden_units,\n",
    "                aggregation_type=\"sum\",\n",
    "                combination_type=\"concat\",\n",
    "                dropout_rate=0.2,\n",
    "                normalize=True,\n",
    "                *args,\n",
    "                **kwargs,):\n",
    "\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "        #여기서부터 모델 생성\n",
    "        #node_features.shape, edges.shape, edge_weights.shape\n",
    "        #(TensorShape([2708, 1433]), (2, 5429), TensorShape([5429]))\n",
    "        input_node_features=keras.layers.Input(shape=(1433,))\n",
    "        input_edges=keras.layers.Input(shape=(2,))\n",
    "        input_edges_weights=keras.layers.Input(shape=(1,))\n",
    "        input_trainx=keras.layers.Input(shape=(1435,), dtype='int64')\n",
    "        \n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(input_node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, input_edges, input_edges_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input)\n",
    "        # Compute logits\n",
    "        output = self.compute_logits(node_embeddings)\n",
    "    \n",
    "        self.model = keras.models.Model(inputs=[input_trainx,input_node_features,input_edges,input_edges_weights], outputs=[output])\n",
    "        \n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, inputs, outputs):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=inputs,\n",
    "        y=outputs,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.15,\n",
    "        #callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2708, 1433]), (2, 5429), TensorShape([5429]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features, edges, edge_weights = graph_info\n",
    "\n",
    "# Set edge_weights to ones if not provided.\n",
    "if edge_weights is None:\n",
    "    edge_weights = tf.ones(shape=edges.shape[1])\n",
    "# Scale edge_weights to sum to 1.\n",
    "edge_weights = edge_weights / tf.math.reduce_sum(edge_weights)\n",
    "node_features.shape, edges.shape, edge_weights.shape\n",
    "#(TensorShape([2708, 1433]), (2, 5429), TensorShape([5429]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2708, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, array([[  21,  905,  906, ..., 2586, 1874, 2707],\n",
      "       [   0,    0,    0, ..., 1874, 1876, 1897]], dtype=int64), <tf.Tensor: shape=(5429,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)>)\n",
      "7\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"graph_conv1\" (type GraphConvLayer).\n\nin user code:\n\n    File \"C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_1692/2946578179.py\", line 101, in call  *\n        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n\n    TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\n\n\nCall arguments received:\n  • inputs=('tf.Tensor(shape=(None, 32), dtype=float32)', 'tf.Tensor(shape=(None, 2), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1692/3909144266.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m gnn_model = GNNModel(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#graph_info=graph_info,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1692/894466567.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, hidden_units, aggregation_type, combination_type, dropout_rate, normalize, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_node_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Apply the first graph conv layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_edges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_edges_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Skip connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"graph_conv1\" (type GraphConvLayer).\n\nin user code:\n\n    File \"C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_1692/2946578179.py\", line 101, in call  *\n        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n\n    TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\n\n\nCall arguments received:\n  • inputs=('tf.Tensor(shape=(None, 32), dtype=float32)', 'tf.Tensor(shape=(None, 2), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)')"
     ]
    }
   ],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "node_features, edges, edge_weights = graph_info\n",
    "\n",
    "print(graph_info)\n",
    "print(num_classes)\n",
    "\n",
    "gnn_model = GNNModel(\n",
    "    #graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "# print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.model.summary()\n",
    "\n",
    "x_train = train_data.paper_id.to_numpy()\n",
    "print(x_train.shape)\n",
    "history = run_experiment(gnn_model.get_model(), [x_train, node_features, edges, edge_weights], [y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2708, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, array([[  21,  905,  906, ..., 2586, 1874, 2707],\n",
      "       [   0,    0,    0, ..., 1874, 1876, 1897]], dtype=int64), <tf.Tensor: shape=(5429,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)>)\n",
      "7\n",
      "GNN output shape: tf.Tensor(\n",
      "[[ 0.07014371 -0.08579759  0.11451342  0.02328166  0.07051581  0.22653823\n",
      "   0.14369097]\n",
      " [ 0.11443309  0.00081104  0.03951072  0.05815473  0.01158473  0.04589193\n",
      "  -0.27310726]\n",
      " [-0.05110506 -0.00064083  0.00840581 -0.027003    0.08705085  0.06186487\n",
      "   0.02247791]], shape=(3, 7), dtype=float32)\n",
      "Model: \"gnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " preprocess (Sequential)     (2708, 32)                52804     \n",
      "                                                                 \n",
      " graph_conv1 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " graph_conv2 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " postprocess (Sequential)    (2708, 32)                2368      \n",
      "                                                                 \n",
      " logits (Dense)              multiple                  231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,179\n",
      "Trainable params: 63,481\n",
      "Non-trainable params: 3,698\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "print(graph_info)\n",
    "print(num_classes)\n",
    "\n",
    "gnn_model = GNNNodeClassifier(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiment(model, x_train, y_train):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1342, 1433), (1342,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 3s 169ms/step - loss: 2.2768 - acc: 0.1575 - val_loss: 1.9281 - val_acc: 0.1232\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.9714 - acc: 0.2576 - val_loss: 1.9221 - val_acc: 0.1429\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.9510 - acc: 0.2689 - val_loss: 1.9142 - val_acc: 0.2020\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.8722 - acc: 0.2785 - val_loss: 1.9089 - val_acc: 0.1576\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.8632 - acc: 0.2846 - val_loss: 1.9113 - val_acc: 0.1330\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.8595 - acc: 0.2750 - val_loss: 1.9018 - val_acc: 0.1675\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.8633 - acc: 0.2950 - val_loss: 1.8809 - val_acc: 0.3054\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 1.8209 - acc: 0.3046 - val_loss: 1.8585 - val_acc: 0.3448\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.7892 - acc: 0.3133 - val_loss: 1.8279 - val_acc: 0.3695\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.7702 - acc: 0.3124 - val_loss: 1.7887 - val_acc: 0.3941\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.7555 - acc: 0.3142 - val_loss: 1.7352 - val_acc: 0.4483\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.7300 - acc: 0.3473 - val_loss: 1.6717 - val_acc: 0.4975\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.6945 - acc: 0.3490 - val_loss: 1.5881 - val_acc: 0.5074\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.6566 - acc: 0.3899 - val_loss: 1.5019 - val_acc: 0.5123\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.6287 - acc: 0.3716 - val_loss: 1.3912 - val_acc: 0.5074\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.5589 - acc: 0.3995 - val_loss: 1.3357 - val_acc: 0.5025\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.5069 - acc: 0.4212 - val_loss: 1.2943 - val_acc: 0.5123\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.5016 - acc: 0.4317 - val_loss: 1.3101 - val_acc: 0.5222\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.4604 - acc: 0.4413 - val_loss: 1.4056 - val_acc: 0.5320\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.3843 - acc: 0.4752 - val_loss: 1.5204 - val_acc: 0.5123\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 1.3372 - acc: 0.4961 - val_loss: 1.5019 - val_acc: 0.5025\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.3425 - acc: 0.4830 - val_loss: 1.5928 - val_acc: 0.5123\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 1.3019 - acc: 0.5213 - val_loss: 1.5283 - val_acc: 0.5567\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.2365 - acc: 0.5292 - val_loss: 1.3960 - val_acc: 0.6010\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.2464 - acc: 0.5309 - val_loss: 1.2110 - val_acc: 0.6108\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.2076 - acc: 0.5527 - val_loss: 1.1626 - val_acc: 0.6305\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.1399 - acc: 0.5970 - val_loss: 1.1688 - val_acc: 0.6010\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.1685 - acc: 0.5779 - val_loss: 1.0993 - val_acc: 0.6059\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.1371 - acc: 0.5788 - val_loss: 0.9883 - val_acc: 0.6847\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.1024 - acc: 0.6023 - val_loss: 1.0244 - val_acc: 0.6700\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.0476 - acc: 0.6214 - val_loss: 1.0325 - val_acc: 0.6700\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.0584 - acc: 0.5997 - val_loss: 1.1133 - val_acc: 0.6552\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.0242 - acc: 0.6327 - val_loss: 1.1125 - val_acc: 0.6650\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.0267 - acc: 0.6258 - val_loss: 1.1525 - val_acc: 0.5911\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.9586 - acc: 0.6562 - val_loss: 1.1389 - val_acc: 0.5862\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.9609 - acc: 0.6701 - val_loss: 1.0858 - val_acc: 0.6305\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.9466 - acc: 0.6780 - val_loss: 0.8962 - val_acc: 0.6995\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.9462 - acc: 0.6867 - val_loss: 0.8638 - val_acc: 0.7340\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.9422 - acc: 0.6728 - val_loss: 0.8345 - val_acc: 0.7340\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.8779 - acc: 0.7058 - val_loss: 0.8247 - val_acc: 0.7192\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.8460 - acc: 0.7015 - val_loss: 0.8423 - val_acc: 0.7044\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.9302 - acc: 0.6902 - val_loss: 0.8754 - val_acc: 0.6897\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.8607 - acc: 0.6963 - val_loss: 0.8343 - val_acc: 0.7094\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.8286 - acc: 0.7171 - val_loss: 0.8074 - val_acc: 0.7192\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7728 - acc: 0.7406 - val_loss: 0.7959 - val_acc: 0.7340\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7889 - acc: 0.7311 - val_loss: 0.7499 - val_acc: 0.7685\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.8037 - acc: 0.7215 - val_loss: 0.8234 - val_acc: 0.7438\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7898 - acc: 0.7450 - val_loss: 0.9253 - val_acc: 0.7143\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.7759 - acc: 0.7372 - val_loss: 0.9566 - val_acc: 0.6946\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.7546 - acc: 0.7380 - val_loss: 0.9141 - val_acc: 0.6995\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.7777 - acc: 0.7337 - val_loss: 0.9343 - val_acc: 0.7044\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7186 - acc: 0.7607 - val_loss: 0.9283 - val_acc: 0.6995\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.6863 - acc: 0.7537 - val_loss: 0.8745 - val_acc: 0.7340\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.7424 - acc: 0.7537 - val_loss: 0.7779 - val_acc: 0.7291\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.7085 - acc: 0.7824 - val_loss: 0.7094 - val_acc: 0.7635\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.7209 - acc: 0.7598 - val_loss: 0.6850 - val_acc: 0.7685\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7099 - acc: 0.7720 - val_loss: 0.6861 - val_acc: 0.7586\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6763 - acc: 0.7641 - val_loss: 0.6603 - val_acc: 0.7586\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.7017 - acc: 0.7563 - val_loss: 0.6820 - val_acc: 0.7537\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7074 - acc: 0.7389 - val_loss: 0.8150 - val_acc: 0.7044\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.6508 - acc: 0.7755 - val_loss: 0.7037 - val_acc: 0.7537\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.6705 - acc: 0.7720 - val_loss: 0.6180 - val_acc: 0.7783\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.6179 - acc: 0.8050 - val_loss: 0.6169 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.6222 - acc: 0.7833 - val_loss: 0.6087 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6181 - acc: 0.7772 - val_loss: 0.6045 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6009 - acc: 0.8033 - val_loss: 0.5947 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6353 - acc: 0.7894 - val_loss: 0.5977 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.6138 - acc: 0.7998 - val_loss: 0.5893 - val_acc: 0.8227\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.5942 - acc: 0.8016 - val_loss: 0.6050 - val_acc: 0.8128\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5923 - acc: 0.8094 - val_loss: 0.6496 - val_acc: 0.7931\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6446 - acc: 0.7850 - val_loss: 0.5768 - val_acc: 0.7980\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6172 - acc: 0.8024 - val_loss: 0.6034 - val_acc: 0.7882\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6190 - acc: 0.7911 - val_loss: 0.5778 - val_acc: 0.7931\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.5884 - acc: 0.7981 - val_loss: 0.6036 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6240 - acc: 0.7929 - val_loss: 0.6290 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5946 - acc: 0.8042 - val_loss: 0.6100 - val_acc: 0.7833\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5932 - acc: 0.7955 - val_loss: 0.5788 - val_acc: 0.7980\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5631 - acc: 0.8120 - val_loss: 0.5879 - val_acc: 0.8030\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.5681 - acc: 0.8216 - val_loss: 0.6227 - val_acc: 0.7931\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5879 - acc: 0.8077 - val_loss: 0.7250 - val_acc: 0.7488\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.6146 - acc: 0.7998 - val_loss: 0.7265 - val_acc: 0.7635\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5065 - acc: 0.8346 - val_loss: 0.6595 - val_acc: 0.7734\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5461 - acc: 0.8138 - val_loss: 0.6444 - val_acc: 0.7635\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.5304 - acc: 0.8372 - val_loss: 0.6075 - val_acc: 0.7783\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5454 - acc: 0.8277 - val_loss: 0.5609 - val_acc: 0.7980\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5366 - acc: 0.8251 - val_loss: 0.5320 - val_acc: 0.7980\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5639 - acc: 0.8138 - val_loss: 0.5272 - val_acc: 0.7931\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.5488 - acc: 0.8111 - val_loss: 0.5915 - val_acc: 0.7980\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5605 - acc: 0.8050 - val_loss: 0.5954 - val_acc: 0.7882\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5201 - acc: 0.8251 - val_loss: 0.5401 - val_acc: 0.8227\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5074 - acc: 0.8477 - val_loss: 0.5197 - val_acc: 0.8424\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5317 - acc: 0.8320 - val_loss: 0.5153 - val_acc: 0.8276\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4610 - acc: 0.8538 - val_loss: 0.5011 - val_acc: 0.8276\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5121 - acc: 0.8277 - val_loss: 0.4985 - val_acc: 0.8276\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4894 - acc: 0.8468 - val_loss: 0.4887 - val_acc: 0.8473\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5047 - acc: 0.8460 - val_loss: 0.4899 - val_acc: 0.8325\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.4642 - acc: 0.8468 - val_loss: 0.5470 - val_acc: 0.7931\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4986 - acc: 0.8390 - val_loss: 0.5111 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4681 - acc: 0.8529 - val_loss: 0.4849 - val_acc: 0.8473\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.5199 - acc: 0.8329 - val_loss: 0.4739 - val_acc: 0.8473\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4905 - acc: 0.8364 - val_loss: 0.5206 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4943 - acc: 0.8425 - val_loss: 0.5311 - val_acc: 0.8227\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4645 - acc: 0.8477 - val_loss: 0.5008 - val_acc: 0.8325\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4805 - acc: 0.8390 - val_loss: 0.4876 - val_acc: 0.8128\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4845 - acc: 0.8477 - val_loss: 0.5243 - val_acc: 0.8128\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4836 - acc: 0.8460 - val_loss: 0.5081 - val_acc: 0.8227\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4945 - acc: 0.8407 - val_loss: 0.4865 - val_acc: 0.8177\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.4769 - acc: 0.8503 - val_loss: 0.4907 - val_acc: 0.8227\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4829 - acc: 0.8529 - val_loss: 0.5065 - val_acc: 0.8177\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4752 - acc: 0.8512 - val_loss: 0.5507 - val_acc: 0.8030\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4535 - acc: 0.8564 - val_loss: 0.5359 - val_acc: 0.8128\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4766 - acc: 0.8468 - val_loss: 0.4942 - val_acc: 0.8276\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4867 - acc: 0.8390 - val_loss: 0.4886 - val_acc: 0.8374\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4571 - acc: 0.8547 - val_loss: 0.4894 - val_acc: 0.8374\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4591 - acc: 0.8433 - val_loss: 0.4756 - val_acc: 0.8374\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4413 - acc: 0.8634 - val_loss: 0.4715 - val_acc: 0.8424\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4608 - acc: 0.8416 - val_loss: 0.4805 - val_acc: 0.8374\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4612 - acc: 0.8677 - val_loss: 0.4469 - val_acc: 0.8473\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4504 - acc: 0.8607 - val_loss: 0.4498 - val_acc: 0.8818\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4931 - acc: 0.8277 - val_loss: 0.4564 - val_acc: 0.8621\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4410 - acc: 0.8599 - val_loss: 0.4835 - val_acc: 0.8424\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4343 - acc: 0.8634 - val_loss: 0.4676 - val_acc: 0.8473\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4283 - acc: 0.8547 - val_loss: 0.4479 - val_acc: 0.8571\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4054 - acc: 0.8668 - val_loss: 0.4226 - val_acc: 0.8670\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4344 - acc: 0.8564 - val_loss: 0.4155 - val_acc: 0.8719\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4261 - acc: 0.8703 - val_loss: 0.4181 - val_acc: 0.8818\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4567 - acc: 0.8581 - val_loss: 0.4182 - val_acc: 0.8719\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4268 - acc: 0.8599 - val_loss: 0.4124 - val_acc: 0.8719\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4396 - acc: 0.8599 - val_loss: 0.4460 - val_acc: 0.8522\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3933 - acc: 0.8764 - val_loss: 0.4521 - val_acc: 0.8522\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.4395 - acc: 0.8747 - val_loss: 0.4605 - val_acc: 0.8522\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4132 - acc: 0.8712 - val_loss: 0.4801 - val_acc: 0.8473\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4596 - acc: 0.8686 - val_loss: 0.4952 - val_acc: 0.8424\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3682 - acc: 0.8755 - val_loss: 0.4553 - val_acc: 0.8522\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4463 - acc: 0.8555 - val_loss: 0.4202 - val_acc: 0.8571\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4490 - acc: 0.8616 - val_loss: 0.4123 - val_acc: 0.8670\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4073 - acc: 0.8782 - val_loss: 0.4349 - val_acc: 0.8768\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4276 - acc: 0.8581 - val_loss: 0.4375 - val_acc: 0.8670\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4002 - acc: 0.8695 - val_loss: 0.4195 - val_acc: 0.8719\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4305 - acc: 0.8668 - val_loss: 0.4322 - val_acc: 0.8867\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4124 - acc: 0.8755 - val_loss: 0.4813 - val_acc: 0.8522\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4328 - acc: 0.8599 - val_loss: 0.5128 - val_acc: 0.8522\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4033 - acc: 0.8782 - val_loss: 0.4990 - val_acc: 0.8571\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3996 - acc: 0.8799 - val_loss: 0.4843 - val_acc: 0.8670\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.4049 - acc: 0.8764 - val_loss: 0.4792 - val_acc: 0.8571\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3669 - acc: 0.8721 - val_loss: 0.4886 - val_acc: 0.8473\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4239 - acc: 0.8703 - val_loss: 0.4851 - val_acc: 0.8522\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4229 - acc: 0.8712 - val_loss: 0.4488 - val_acc: 0.8719\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4030 - acc: 0.8808 - val_loss: 0.4259 - val_acc: 0.8719\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3495 - acc: 0.8877 - val_loss: 0.4303 - val_acc: 0.8719\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3920 - acc: 0.8677 - val_loss: 0.4509 - val_acc: 0.8571\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.3964 - acc: 0.8721 - val_loss: 0.4770 - val_acc: 0.8325\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3700 - acc: 0.8790 - val_loss: 0.4608 - val_acc: 0.8473\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4146 - acc: 0.8747 - val_loss: 0.4563 - val_acc: 0.8473\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3656 - acc: 0.8851 - val_loss: 0.4633 - val_acc: 0.8424\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4351 - acc: 0.8729 - val_loss: 0.4666 - val_acc: 0.8522\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.3835 - acc: 0.8799 - val_loss: 0.4705 - val_acc: 0.8473\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.3929 - acc: 0.8764 - val_loss: 0.4893 - val_acc: 0.8276\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4144 - acc: 0.8660 - val_loss: 0.5160 - val_acc: 0.8374\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3975 - acc: 0.8712 - val_loss: 0.5426 - val_acc: 0.8325\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3684 - acc: 0.8930 - val_loss: 0.4897 - val_acc: 0.8276\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3660 - acc: 0.8834 - val_loss: 0.4569 - val_acc: 0.8621\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4077 - acc: 0.8642 - val_loss: 0.4404 - val_acc: 0.8768\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3745 - acc: 0.8877 - val_loss: 0.4556 - val_acc: 0.8621\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3897 - acc: 0.8695 - val_loss: 0.5059 - val_acc: 0.8325\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3597 - acc: 0.8860 - val_loss: 0.4770 - val_acc: 0.8374\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3867 - acc: 0.8703 - val_loss: 0.4582 - val_acc: 0.8522\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4303 - acc: 0.8642 - val_loss: 0.4581 - val_acc: 0.8522\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3512 - acc: 0.8903 - val_loss: 0.4785 - val_acc: 0.8571\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3810 - acc: 0.8773 - val_loss: 0.4871 - val_acc: 0.8424\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4062 - acc: 0.8695 - val_loss: 0.4871 - val_acc: 0.8473\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3768 - acc: 0.8842 - val_loss: 0.5106 - val_acc: 0.8473\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3572 - acc: 0.8930 - val_loss: 0.5345 - val_acc: 0.8424\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4093 - acc: 0.8738 - val_loss: 0.5260 - val_acc: 0.8276\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3950 - acc: 0.8677 - val_loss: 0.4876 - val_acc: 0.8424\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3483 - acc: 0.8930 - val_loss: 0.4446 - val_acc: 0.8571\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3605 - acc: 0.8782 - val_loss: 0.4700 - val_acc: 0.8424\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3549 - acc: 0.8964 - val_loss: 0.4603 - val_acc: 0.8424\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3934 - acc: 0.8747 - val_loss: 0.4540 - val_acc: 0.8621\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.4026 - acc: 0.8851 - val_loss: 0.4614 - val_acc: 0.8719\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3465 - acc: 0.8869 - val_loss: 0.4787 - val_acc: 0.8571\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3832 - acc: 0.8808 - val_loss: 0.5070 - val_acc: 0.8374\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3253 - acc: 0.8912 - val_loss: 0.4619 - val_acc: 0.8424\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3325 - acc: 0.9008 - val_loss: 0.4429 - val_acc: 0.8522\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3897 - acc: 0.8729 - val_loss: 0.4431 - val_acc: 0.8522\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3456 - acc: 0.8956 - val_loss: 0.4602 - val_acc: 0.8424\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3702 - acc: 0.8921 - val_loss: 0.4619 - val_acc: 0.8424\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3894 - acc: 0.8947 - val_loss: 0.4486 - val_acc: 0.8473\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3577 - acc: 0.8869 - val_loss: 0.4729 - val_acc: 0.8473\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3489 - acc: 0.8877 - val_loss: 0.4936 - val_acc: 0.8473\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data.paper_id.to_numpy()\n",
    "history = run_experiment(gnn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 30.53%\n"
     ]
    }
   ],
   "source": [
    "x_test = test_data.paper_id.to_numpy()\n",
    "_, test_accuracy = gnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = loader.class_values\n",
    "node_features=loader.node_features\n",
    "edges = loader.edges\n",
    "\n",
    "def display_class_probabilities(probabilities):\n",
    "    for instance_idx, probs in enumerate(probabilities):\n",
    "        print(f\"Instance {instance_idx + 1}:\")\n",
    "        for class_idx, prob in enumerate(probs):\n",
    "            print(f\"- {class_values[class_idx]}: {round(prob * 100, 2)}%\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GNNModel' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5500/1116990751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2708\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2709\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2710\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices[:2]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay_class_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GNNModel' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "logits = gnn_model.predict(tf.constant([2708, 2709, 2710]))\n",
    "# logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices[:2]))\n",
    "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
    "display_class_probabilities(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5812e37fbc42ae0019c075dcd625ea6adf837b197758e07cfdfe5b415c77a600"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
