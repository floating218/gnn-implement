{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_dataset(self):    \n",
    "        \n",
    "        '''\n",
    "        citations: [target논문인덱스, source논문인덱스]\n",
    "        papers: [논문인덱스, 1424개 단어 포함 여부, 주제(subject)]\n",
    "        train_data: papers 데이터 중 50% 샘플링\n",
    "        test_data: papers 데이터 중 50% 샘플링\n",
    "        x_train: train_data 중, 논문인덱스와 subject를 제외한 피쳐\n",
    "        y_train: train_data 중 subject에 해당하는 레이블\n",
    "        '''\n",
    "        \n",
    "        zip_file = keras.utils.get_file(\n",
    "            fname=\"cora.tgz\",\n",
    "            origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "            extract=True,\n",
    "        )\n",
    "        data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "        \n",
    "        citations = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.cites\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"target\", \"source\"],\n",
    "        )\n",
    "        \n",
    "        column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "        papers = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.content\"), \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=column_names,\n",
    "        )\n",
    "        \n",
    "        class_values = sorted(papers[\"subject\"].unique())\n",
    "        class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "        paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "        papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "        papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "        \n",
    "        train_data, test_data = [], []\n",
    "\n",
    "        for _, group_data in papers.groupby(\"subject\"):\n",
    "            # Select around 50% of the dataset for training.\n",
    "            random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "            train_data.append(group_data[random_selection])\n",
    "            test_data.append(group_data[~random_selection])\n",
    "\n",
    "        train_data = pd.concat(train_data).sample(frac=1)\n",
    "        test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "        print(\"citations data shape:\", citations.shape)\n",
    "        print(\"papers data shape:\", papers.shape)\n",
    "        print(\"Train data shape:\", train_data.shape)\n",
    "        print(\"Test data shape:\", test_data.shape)\n",
    "        \n",
    "        feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
    "        num_features = len(feature_names)\n",
    "        num_classes = len(class_idx)\n",
    "\n",
    "        # Create train and test features as a numpy array.\n",
    "        x_train = train_data[feature_names].to_numpy()\n",
    "        x_test = test_data[feature_names].to_numpy()\n",
    "        \n",
    "        # Create train and test targets as a numpy array.\n",
    "        y_train = train_data[\"subject\"]\n",
    "        y_test = test_data[\"subject\"]\n",
    "        \n",
    "        # Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
    "        edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "        # Create an edge weights array of ones.\n",
    "        edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Create a node features array of shape [num_nodes, num_features].\n",
    "        node_features = tf.cast(\n",
    "            papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    "        )\n",
    "        # Create graph info tuple with node_features, edges, and edge_weights.\n",
    "        graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "        print(\"Edges shape:\", edges.shape)\n",
    "        print(\"Nodes shape:\", node_features.shape)\n",
    "        \n",
    "        self.feature_names = feature_names #단어 피쳐 컬럼 이름\n",
    "        self.num_features = num_features #단어 피쳐 개수\n",
    "        self.num_classes = num_classes #주제subject의 가짓수\n",
    "        self.graph_info = graph_info\n",
    "        self.node_features = node_features #논문 인덱스 순으로 정리된 상태의 feature 매트릭스\n",
    "        self.edges = edges\n",
    "        self.class_values = class_values\n",
    "        \n",
    "        return citations, papers, x_train, x_test, y_train, y_test, train_data, test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citations data shape: (5429, 2)\n",
      "papers data shape: (2708, 1435)\n",
      "Train data shape: (1342, 1435)\n",
      "Test data shape: (1366, 1435)\n",
      "Edges shape: (2, 5429)\n",
      "Nodes shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "loader = Loader()\n",
    "citations, papers, x_train, x_test, y_train, y_test, train_data, test_data = loader.load_dataset()\n",
    "feature_names = loader.feature_names\n",
    "num_classes = loader.num_classes\n",
    "graph_info = loader.graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    '''\n",
    "hidden_units = [32, 32]\n",
    "dropout_rate = 0.2\n",
    "'''\n",
    "\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(layers.Layer):\n",
    "    '''\n",
    "    1.gather: 이웃논문별 특성을 모음\n",
    "    2.prepare: 이웃논문별 특성을 ffn한 후, edge점수를 곱함\n",
    "    3.aggregate: 전체논문별로 이웃논문들의 결과를 aggregate함\n",
    "    4.update1: 전체논문별 특성과 위 결과를 concat\n",
    "    5.update2: concat한 것을 ffn해서 최종 embedding도출\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel:\n",
    "    def __init__(self,\n",
    "                graph_info,\n",
    "                num_classes,\n",
    "                hidden_units,\n",
    "                aggregation_type=\"sum\",\n",
    "                combination_type=\"concat\",\n",
    "                dropout_rate=0.2,\n",
    "                normalize=True,\n",
    "                *args,\n",
    "                **kwargs,):\n",
    "        \n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, self.edges, self.edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "        #여기서부터 모델 생성\n",
    "        input=keras.layers.Input(shape=(1435,), dtype='int64')\n",
    "        \n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input)\n",
    "        # Compute logits\n",
    "        output = self.compute_logits(node_embeddings)\n",
    "    \n",
    "        self.model = keras.models.Model([input], [output])\n",
    "        \n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, x_train, y_train):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.15,\n",
    "        #callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2708, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, array([[  21,  905,  906, ..., 2586, 1874, 2707],\n",
      "       [   0,    0,    0, ..., 1874, 1876, 1897]], dtype=int64), <tf.Tensor: shape=(5429,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)>)\n",
      "7\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 1435)]            0         \n",
      "                                                                 \n",
      " tf.compat.v1.gather_14 (TFO  (None, 1435, 32)         0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " logits (Dense)              (None, 1435, 7)           231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 231\n",
      "Trainable params: 231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "print(graph_info)\n",
    "print(num_classes)\n",
    "\n",
    "gnn_model = GNNModel(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "# print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342,)\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1435) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1435), dtype=tf.int64, name='input_17'), name='input_17', description=\"created by layer 'input_17'\"), but it was called on an input with incompatible shape (None,).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1435) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1435), dtype=tf.int64, name='input_17'), name='input_17', description=\"created by layer 'input_17'\"), but it was called on an input with incompatible shape (None,).\n",
      " 1/36 [..............................] - ETA: 5s - loss: 1.9471 - acc: 0.0938WARNING:tensorflow:Model was constructed with shape (None, 1435) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1435), dtype=tf.int64, name='input_17'), name='input_17', description=\"created by layer 'input_17'\"), but it was called on an input with incompatible shape (None,).\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.8959 - acc: 0.2342 - val_loss: 1.8355 - val_acc: 0.3020\n",
      "Epoch 2/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.8180 - acc: 0.3044 - val_loss: 1.7920 - val_acc: 0.3069\n",
      "Epoch 3/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.7877 - acc: 0.3061 - val_loss: 1.7702 - val_acc: 0.3069\n",
      "Epoch 4/300\n",
      "36/36 [==============================] - 0s 979us/step - loss: 1.7728 - acc: 0.3088 - val_loss: 1.7591 - val_acc: 0.3069\n",
      "Epoch 5/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7617 - acc: 0.3158 - val_loss: 1.7517 - val_acc: 0.3119\n",
      "Epoch 6/300\n",
      "36/36 [==============================] - 0s 974us/step - loss: 1.7541 - acc: 0.3184 - val_loss: 1.7434 - val_acc: 0.3267\n",
      "Epoch 7/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7469 - acc: 0.3123 - val_loss: 1.7423 - val_acc: 0.3267\n",
      "Epoch 8/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7400 - acc: 0.3211 - val_loss: 1.7369 - val_acc: 0.3317\n",
      "Epoch 9/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7352 - acc: 0.3246 - val_loss: 1.7357 - val_acc: 0.3317\n",
      "Epoch 10/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7318 - acc: 0.3272 - val_loss: 1.7331 - val_acc: 0.3317\n",
      "Epoch 11/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7275 - acc: 0.3281 - val_loss: 1.7306 - val_acc: 0.3020\n",
      "Epoch 12/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7237 - acc: 0.3272 - val_loss: 1.7295 - val_acc: 0.3119\n",
      "Epoch 13/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7209 - acc: 0.3333 - val_loss: 1.7298 - val_acc: 0.2970\n",
      "Epoch 14/300\n",
      "36/36 [==============================] - 0s 990us/step - loss: 1.7182 - acc: 0.3316 - val_loss: 1.7264 - val_acc: 0.3020\n",
      "Epoch 15/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7157 - acc: 0.3325 - val_loss: 1.7297 - val_acc: 0.3020\n",
      "Epoch 16/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.7127 - acc: 0.3368 - val_loss: 1.7248 - val_acc: 0.3069\n",
      "Epoch 17/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7106 - acc: 0.3307 - val_loss: 1.7261 - val_acc: 0.3020\n",
      "Epoch 18/300\n",
      "36/36 [==============================] - 0s 984us/step - loss: 1.7082 - acc: 0.3360 - val_loss: 1.7264 - val_acc: 0.3119\n",
      "Epoch 19/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7068 - acc: 0.3404 - val_loss: 1.7251 - val_acc: 0.3119\n",
      "Epoch 20/300\n",
      "36/36 [==============================] - 0s 983us/step - loss: 1.7047 - acc: 0.3404 - val_loss: 1.7244 - val_acc: 0.3069\n",
      "Epoch 21/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.7027 - acc: 0.3377 - val_loss: 1.7272 - val_acc: 0.3119\n",
      "Epoch 22/300\n",
      "36/36 [==============================] - 0s 978us/step - loss: 1.7015 - acc: 0.3447 - val_loss: 1.7236 - val_acc: 0.3119\n",
      "Epoch 23/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7007 - acc: 0.3360 - val_loss: 1.7253 - val_acc: 0.3168\n",
      "Epoch 24/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6993 - acc: 0.3404 - val_loss: 1.7245 - val_acc: 0.3119\n",
      "Epoch 25/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6976 - acc: 0.3447 - val_loss: 1.7247 - val_acc: 0.3119\n",
      "Epoch 26/300\n",
      "36/36 [==============================] - 0s 965us/step - loss: 1.6965 - acc: 0.3430 - val_loss: 1.7247 - val_acc: 0.3119\n",
      "Epoch 27/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6947 - acc: 0.3456 - val_loss: 1.7222 - val_acc: 0.3218\n",
      "Epoch 28/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6930 - acc: 0.3439 - val_loss: 1.7233 - val_acc: 0.3168\n",
      "Epoch 29/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6926 - acc: 0.3439 - val_loss: 1.7239 - val_acc: 0.3119\n",
      "Epoch 30/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6917 - acc: 0.3404 - val_loss: 1.7233 - val_acc: 0.3168\n",
      "Epoch 31/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6908 - acc: 0.3430 - val_loss: 1.7243 - val_acc: 0.3317\n",
      "Epoch 32/300\n",
      "36/36 [==============================] - 0s 994us/step - loss: 1.6896 - acc: 0.3474 - val_loss: 1.7227 - val_acc: 0.3168\n",
      "Epoch 33/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6887 - acc: 0.3465 - val_loss: 1.7233 - val_acc: 0.3267\n",
      "Epoch 34/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6879 - acc: 0.3474 - val_loss: 1.7237 - val_acc: 0.3366\n",
      "Epoch 35/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6868 - acc: 0.3474 - val_loss: 1.7257 - val_acc: 0.3267\n",
      "Epoch 36/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6856 - acc: 0.3439 - val_loss: 1.7228 - val_acc: 0.3317\n",
      "Epoch 37/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6848 - acc: 0.3456 - val_loss: 1.7247 - val_acc: 0.3416\n",
      "Epoch 38/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6846 - acc: 0.3535 - val_loss: 1.7252 - val_acc: 0.3267\n",
      "Epoch 39/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6837 - acc: 0.3518 - val_loss: 1.7228 - val_acc: 0.3218\n",
      "Epoch 40/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6835 - acc: 0.3412 - val_loss: 1.7240 - val_acc: 0.3218\n",
      "Epoch 41/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6826 - acc: 0.3509 - val_loss: 1.7249 - val_acc: 0.3267\n",
      "Epoch 42/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6811 - acc: 0.3456 - val_loss: 1.7229 - val_acc: 0.3218\n",
      "Epoch 43/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6813 - acc: 0.3491 - val_loss: 1.7230 - val_acc: 0.3267\n",
      "Epoch 44/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6804 - acc: 0.3474 - val_loss: 1.7254 - val_acc: 0.3317\n",
      "Epoch 45/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6797 - acc: 0.3518 - val_loss: 1.7232 - val_acc: 0.3218\n",
      "Epoch 46/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6789 - acc: 0.3509 - val_loss: 1.7240 - val_acc: 0.3218\n",
      "Epoch 47/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6790 - acc: 0.3553 - val_loss: 1.7251 - val_acc: 0.3168\n",
      "Epoch 48/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6777 - acc: 0.3526 - val_loss: 1.7259 - val_acc: 0.3267\n",
      "Epoch 49/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6770 - acc: 0.3561 - val_loss: 1.7243 - val_acc: 0.3218\n",
      "Epoch 50/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6762 - acc: 0.3544 - val_loss: 1.7246 - val_acc: 0.3267\n",
      "Epoch 51/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6772 - acc: 0.3544 - val_loss: 1.7237 - val_acc: 0.3267\n",
      "Epoch 52/300\n",
      "36/36 [==============================] - 0s 986us/step - loss: 1.6758 - acc: 0.3553 - val_loss: 1.7262 - val_acc: 0.3267\n",
      "Epoch 53/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6759 - acc: 0.3491 - val_loss: 1.7241 - val_acc: 0.3317\n",
      "Epoch 54/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6753 - acc: 0.3535 - val_loss: 1.7276 - val_acc: 0.3267\n",
      "Epoch 55/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6757 - acc: 0.3509 - val_loss: 1.7250 - val_acc: 0.3168\n",
      "Epoch 56/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6732 - acc: 0.3535 - val_loss: 1.7235 - val_acc: 0.3168\n",
      "Epoch 57/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6736 - acc: 0.3570 - val_loss: 1.7270 - val_acc: 0.3267\n",
      "Epoch 58/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6731 - acc: 0.3465 - val_loss: 1.7258 - val_acc: 0.3366\n",
      "Epoch 59/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6717 - acc: 0.3553 - val_loss: 1.7241 - val_acc: 0.3218\n",
      "Epoch 60/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6712 - acc: 0.3500 - val_loss: 1.7256 - val_acc: 0.3267\n",
      "Epoch 61/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6707 - acc: 0.3535 - val_loss: 1.7259 - val_acc: 0.3267\n",
      "Epoch 62/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6711 - acc: 0.3544 - val_loss: 1.7263 - val_acc: 0.3218\n",
      "Epoch 63/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6702 - acc: 0.3570 - val_loss: 1.7244 - val_acc: 0.3267\n",
      "Epoch 64/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6700 - acc: 0.3553 - val_loss: 1.7247 - val_acc: 0.3267\n",
      "Epoch 65/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6688 - acc: 0.3535 - val_loss: 1.7279 - val_acc: 0.3218\n",
      "Epoch 66/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6699 - acc: 0.3579 - val_loss: 1.7269 - val_acc: 0.3119\n",
      "Epoch 67/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6686 - acc: 0.3588 - val_loss: 1.7267 - val_acc: 0.3267\n",
      "Epoch 68/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6682 - acc: 0.3596 - val_loss: 1.7269 - val_acc: 0.3267\n",
      "Epoch 69/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6683 - acc: 0.3535 - val_loss: 1.7232 - val_acc: 0.3218\n",
      "Epoch 70/300\n",
      "36/36 [==============================] - 0s 996us/step - loss: 1.6673 - acc: 0.3544 - val_loss: 1.7258 - val_acc: 0.3218\n",
      "Epoch 71/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6664 - acc: 0.3588 - val_loss: 1.7243 - val_acc: 0.3218\n",
      "Epoch 72/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6667 - acc: 0.3596 - val_loss: 1.7258 - val_acc: 0.3267\n",
      "Epoch 73/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6666 - acc: 0.3570 - val_loss: 1.7256 - val_acc: 0.3218\n",
      "Epoch 74/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6663 - acc: 0.3561 - val_loss: 1.7259 - val_acc: 0.3218\n",
      "Epoch 75/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6655 - acc: 0.3605 - val_loss: 1.7254 - val_acc: 0.3218\n",
      "Epoch 76/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6664 - acc: 0.3588 - val_loss: 1.7263 - val_acc: 0.3218\n",
      "Epoch 77/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6666 - acc: 0.3561 - val_loss: 1.7259 - val_acc: 0.3218\n",
      "Epoch 78/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6650 - acc: 0.3544 - val_loss: 1.7261 - val_acc: 0.3366\n",
      "Epoch 79/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6644 - acc: 0.3588 - val_loss: 1.7262 - val_acc: 0.3218\n",
      "Epoch 80/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6641 - acc: 0.3640 - val_loss: 1.7260 - val_acc: 0.3267\n",
      "Epoch 81/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6638 - acc: 0.3623 - val_loss: 1.7277 - val_acc: 0.3069\n",
      "Epoch 82/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6631 - acc: 0.3614 - val_loss: 1.7262 - val_acc: 0.3168\n",
      "Epoch 83/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6635 - acc: 0.3544 - val_loss: 1.7271 - val_acc: 0.3366\n",
      "Epoch 84/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6631 - acc: 0.3561 - val_loss: 1.7271 - val_acc: 0.3218\n",
      "Epoch 85/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6633 - acc: 0.3553 - val_loss: 1.7263 - val_acc: 0.3218\n",
      "Epoch 86/300\n",
      "36/36 [==============================] - 0s 974us/step - loss: 1.6628 - acc: 0.3579 - val_loss: 1.7288 - val_acc: 0.3218\n",
      "Epoch 87/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6625 - acc: 0.3570 - val_loss: 1.7277 - val_acc: 0.3317\n",
      "Epoch 88/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6627 - acc: 0.3623 - val_loss: 1.7305 - val_acc: 0.3020\n",
      "Epoch 89/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6621 - acc: 0.3588 - val_loss: 1.7271 - val_acc: 0.3168\n",
      "Epoch 90/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6621 - acc: 0.3605 - val_loss: 1.7278 - val_acc: 0.3218\n",
      "Epoch 91/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6608 - acc: 0.3640 - val_loss: 1.7279 - val_acc: 0.3168\n",
      "Epoch 92/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6610 - acc: 0.3632 - val_loss: 1.7282 - val_acc: 0.3168\n",
      "Epoch 93/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6602 - acc: 0.3632 - val_loss: 1.7289 - val_acc: 0.3218\n",
      "Epoch 94/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6609 - acc: 0.3658 - val_loss: 1.7293 - val_acc: 0.3218\n",
      "Epoch 95/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6600 - acc: 0.3658 - val_loss: 1.7283 - val_acc: 0.3218\n",
      "Epoch 96/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6601 - acc: 0.3667 - val_loss: 1.7298 - val_acc: 0.3069\n",
      "Epoch 97/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6600 - acc: 0.3605 - val_loss: 1.7292 - val_acc: 0.3168\n",
      "Epoch 98/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6592 - acc: 0.3649 - val_loss: 1.7301 - val_acc: 0.3119\n",
      "Epoch 99/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6583 - acc: 0.3614 - val_loss: 1.7295 - val_acc: 0.3218\n",
      "Epoch 100/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6594 - acc: 0.3667 - val_loss: 1.7289 - val_acc: 0.3119\n",
      "Epoch 101/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6594 - acc: 0.3711 - val_loss: 1.7307 - val_acc: 0.3218\n",
      "Epoch 102/300\n",
      "36/36 [==============================] - 0s 974us/step - loss: 1.6584 - acc: 0.3702 - val_loss: 1.7301 - val_acc: 0.3069\n",
      "Epoch 103/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6579 - acc: 0.3658 - val_loss: 1.7295 - val_acc: 0.3069\n",
      "Epoch 104/300\n",
      "36/36 [==============================] - 0s 978us/step - loss: 1.6577 - acc: 0.3667 - val_loss: 1.7291 - val_acc: 0.3168\n",
      "Epoch 105/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6582 - acc: 0.3632 - val_loss: 1.7286 - val_acc: 0.3119\n",
      "Epoch 106/300\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.6891 - acc: 0.312 - 0s 986us/step - loss: 1.6582 - acc: 0.3640 - val_loss: 1.7290 - val_acc: 0.3119\n",
      "Epoch 107/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6570 - acc: 0.3667 - val_loss: 1.7313 - val_acc: 0.3267\n",
      "Epoch 108/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6569 - acc: 0.3684 - val_loss: 1.7314 - val_acc: 0.3168\n",
      "Epoch 109/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6568 - acc: 0.3675 - val_loss: 1.7308 - val_acc: 0.2970\n",
      "Epoch 110/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6567 - acc: 0.3684 - val_loss: 1.7283 - val_acc: 0.3020\n",
      "Epoch 111/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6565 - acc: 0.3675 - val_loss: 1.7316 - val_acc: 0.3069\n",
      "Epoch 112/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6557 - acc: 0.3658 - val_loss: 1.7316 - val_acc: 0.3119\n",
      "Epoch 113/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6570 - acc: 0.3640 - val_loss: 1.7304 - val_acc: 0.3119\n",
      "Epoch 114/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6552 - acc: 0.3693 - val_loss: 1.7307 - val_acc: 0.2921\n",
      "Epoch 115/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6559 - acc: 0.3719 - val_loss: 1.7323 - val_acc: 0.3020\n",
      "Epoch 116/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6554 - acc: 0.3675 - val_loss: 1.7301 - val_acc: 0.3119\n",
      "Epoch 117/300\n",
      "36/36 [==============================] - 0s 1000us/step - loss: 1.6551 - acc: 0.3711 - val_loss: 1.7307 - val_acc: 0.3168\n",
      "Epoch 118/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6553 - acc: 0.3737 - val_loss: 1.7334 - val_acc: 0.3119\n",
      "Epoch 119/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6545 - acc: 0.3711 - val_loss: 1.7305 - val_acc: 0.3069\n",
      "Epoch 120/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6547 - acc: 0.3658 - val_loss: 1.7313 - val_acc: 0.3119\n",
      "Epoch 121/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6543 - acc: 0.3693 - val_loss: 1.7314 - val_acc: 0.3119\n",
      "Epoch 122/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6545 - acc: 0.3746 - val_loss: 1.7314 - val_acc: 0.3069\n",
      "Epoch 123/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6544 - acc: 0.3702 - val_loss: 1.7339 - val_acc: 0.3020\n",
      "Epoch 124/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6542 - acc: 0.3667 - val_loss: 1.7343 - val_acc: 0.2970\n",
      "Epoch 125/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6543 - acc: 0.3737 - val_loss: 1.7337 - val_acc: 0.2970\n",
      "Epoch 126/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6542 - acc: 0.3737 - val_loss: 1.7343 - val_acc: 0.3020\n",
      "Epoch 127/300\n",
      "36/36 [==============================] - 0s 993us/step - loss: 1.6531 - acc: 0.3719 - val_loss: 1.7316 - val_acc: 0.3020\n",
      "Epoch 128/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6535 - acc: 0.3658 - val_loss: 1.7311 - val_acc: 0.3020\n",
      "Epoch 129/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6529 - acc: 0.3754 - val_loss: 1.7342 - val_acc: 0.3020\n",
      "Epoch 130/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6534 - acc: 0.3763 - val_loss: 1.7328 - val_acc: 0.2921\n",
      "Epoch 131/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6516 - acc: 0.3763 - val_loss: 1.7340 - val_acc: 0.3069\n",
      "Epoch 132/300\n",
      "36/36 [==============================] - 0s 982us/step - loss: 1.6522 - acc: 0.3693 - val_loss: 1.7315 - val_acc: 0.2970\n",
      "Epoch 133/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6528 - acc: 0.3719 - val_loss: 1.7332 - val_acc: 0.2921\n",
      "Epoch 134/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6525 - acc: 0.3728 - val_loss: 1.7354 - val_acc: 0.3069\n",
      "Epoch 135/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6523 - acc: 0.3675 - val_loss: 1.7322 - val_acc: 0.3020\n",
      "Epoch 136/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6521 - acc: 0.3728 - val_loss: 1.7345 - val_acc: 0.2871\n",
      "Epoch 137/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6513 - acc: 0.3737 - val_loss: 1.7338 - val_acc: 0.2921\n",
      "Epoch 138/300\n",
      "36/36 [==============================] - 0s 960us/step - loss: 1.6522 - acc: 0.3754 - val_loss: 1.7341 - val_acc: 0.2970\n",
      "Epoch 139/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6508 - acc: 0.3737 - val_loss: 1.7336 - val_acc: 0.3069\n",
      "Epoch 140/300\n",
      "36/36 [==============================] - 0s 968us/step - loss: 1.6509 - acc: 0.3746 - val_loss: 1.7349 - val_acc: 0.3069\n",
      "Epoch 141/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6507 - acc: 0.3763 - val_loss: 1.7338 - val_acc: 0.3020\n",
      "Epoch 142/300\n",
      "36/36 [==============================] - 0s 957us/step - loss: 1.6517 - acc: 0.3693 - val_loss: 1.7337 - val_acc: 0.2970\n",
      "Epoch 143/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6504 - acc: 0.3702 - val_loss: 1.7352 - val_acc: 0.2970\n",
      "Epoch 144/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6506 - acc: 0.3693 - val_loss: 1.7345 - val_acc: 0.2921\n",
      "Epoch 145/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6507 - acc: 0.3719 - val_loss: 1.7351 - val_acc: 0.2970\n",
      "Epoch 146/300\n",
      "36/36 [==============================] - 0s 984us/step - loss: 1.6509 - acc: 0.3754 - val_loss: 1.7343 - val_acc: 0.2921\n",
      "Epoch 147/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6497 - acc: 0.3763 - val_loss: 1.7359 - val_acc: 0.2970\n",
      "Epoch 148/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6503 - acc: 0.3798 - val_loss: 1.7359 - val_acc: 0.2921\n",
      "Epoch 149/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6499 - acc: 0.3763 - val_loss: 1.7357 - val_acc: 0.2921\n",
      "Epoch 150/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6495 - acc: 0.3763 - val_loss: 1.7353 - val_acc: 0.3020\n",
      "Epoch 151/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6488 - acc: 0.3754 - val_loss: 1.7346 - val_acc: 0.2921\n",
      "Epoch 152/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6489 - acc: 0.3772 - val_loss: 1.7349 - val_acc: 0.2871\n",
      "Epoch 153/300\n",
      "36/36 [==============================] - 0s 994us/step - loss: 1.6492 - acc: 0.3711 - val_loss: 1.7378 - val_acc: 0.2970\n",
      "Epoch 154/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6488 - acc: 0.3737 - val_loss: 1.7378 - val_acc: 0.2921\n",
      "Epoch 155/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6487 - acc: 0.3763 - val_loss: 1.7351 - val_acc: 0.2970\n",
      "Epoch 156/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6488 - acc: 0.3737 - val_loss: 1.7341 - val_acc: 0.2921\n",
      "Epoch 157/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6489 - acc: 0.3754 - val_loss: 1.7354 - val_acc: 0.2921\n",
      "Epoch 158/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6482 - acc: 0.3728 - val_loss: 1.7364 - val_acc: 0.2921\n",
      "Epoch 159/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6495 - acc: 0.3746 - val_loss: 1.7360 - val_acc: 0.2921\n",
      "Epoch 160/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6480 - acc: 0.3728 - val_loss: 1.7378 - val_acc: 0.2970\n",
      "Epoch 161/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6490 - acc: 0.3746 - val_loss: 1.7357 - val_acc: 0.2970\n",
      "Epoch 162/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6481 - acc: 0.3772 - val_loss: 1.7369 - val_acc: 0.2921\n",
      "Epoch 163/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6485 - acc: 0.3754 - val_loss: 1.7374 - val_acc: 0.2921\n",
      "Epoch 164/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6481 - acc: 0.3746 - val_loss: 1.7378 - val_acc: 0.3020\n",
      "Epoch 165/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6482 - acc: 0.3772 - val_loss: 1.7368 - val_acc: 0.2921\n",
      "Epoch 166/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6476 - acc: 0.3746 - val_loss: 1.7367 - val_acc: 0.2970\n",
      "Epoch 167/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6471 - acc: 0.3763 - val_loss: 1.7374 - val_acc: 0.2921\n",
      "Epoch 168/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6474 - acc: 0.3719 - val_loss: 1.7380 - val_acc: 0.2921\n",
      "Epoch 169/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6471 - acc: 0.3719 - val_loss: 1.7351 - val_acc: 0.2871\n",
      "Epoch 170/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6469 - acc: 0.3781 - val_loss: 1.7375 - val_acc: 0.2871\n",
      "Epoch 171/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6471 - acc: 0.3763 - val_loss: 1.7376 - val_acc: 0.2871\n",
      "Epoch 172/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6467 - acc: 0.3789 - val_loss: 1.7370 - val_acc: 0.2871\n",
      "Epoch 173/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6472 - acc: 0.3772 - val_loss: 1.7397 - val_acc: 0.2921\n",
      "Epoch 174/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6476 - acc: 0.3763 - val_loss: 1.7370 - val_acc: 0.2921\n",
      "Epoch 175/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6465 - acc: 0.3789 - val_loss: 1.7397 - val_acc: 0.2921\n",
      "Epoch 176/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6469 - acc: 0.3746 - val_loss: 1.7382 - val_acc: 0.2871\n",
      "Epoch 177/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6459 - acc: 0.3754 - val_loss: 1.7397 - val_acc: 0.2970\n",
      "Epoch 178/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6465 - acc: 0.3719 - val_loss: 1.7376 - val_acc: 0.2871\n",
      "Epoch 179/300\n",
      "36/36 [==============================] - 0s 995us/step - loss: 1.6459 - acc: 0.3772 - val_loss: 1.7408 - val_acc: 0.2921\n",
      "Epoch 180/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6457 - acc: 0.3798 - val_loss: 1.7387 - val_acc: 0.2921\n",
      "Epoch 181/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6457 - acc: 0.3763 - val_loss: 1.7383 - val_acc: 0.2871\n",
      "Epoch 182/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6458 - acc: 0.3737 - val_loss: 1.7387 - val_acc: 0.2871\n",
      "Epoch 183/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6446 - acc: 0.3754 - val_loss: 1.7387 - val_acc: 0.2871\n",
      "Epoch 184/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6455 - acc: 0.3746 - val_loss: 1.7391 - val_acc: 0.2871\n",
      "Epoch 185/300\n",
      "36/36 [==============================] - 0s 971us/step - loss: 1.6449 - acc: 0.3781 - val_loss: 1.7393 - val_acc: 0.2822\n",
      "Epoch 186/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6448 - acc: 0.3719 - val_loss: 1.7407 - val_acc: 0.2921\n",
      "Epoch 187/300\n",
      "36/36 [==============================] - 0s 993us/step - loss: 1.6448 - acc: 0.3763 - val_loss: 1.7402 - val_acc: 0.2871\n",
      "Epoch 188/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6446 - acc: 0.3772 - val_loss: 1.7402 - val_acc: 0.2970\n",
      "Epoch 189/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6447 - acc: 0.3719 - val_loss: 1.7391 - val_acc: 0.2921\n",
      "Epoch 190/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6448 - acc: 0.3772 - val_loss: 1.7406 - val_acc: 0.2871\n",
      "Epoch 191/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6444 - acc: 0.3746 - val_loss: 1.7410 - val_acc: 0.2921\n",
      "Epoch 192/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6443 - acc: 0.3763 - val_loss: 1.7416 - val_acc: 0.2921\n",
      "Epoch 193/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6442 - acc: 0.3789 - val_loss: 1.7416 - val_acc: 0.2970\n",
      "Epoch 194/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6449 - acc: 0.3754 - val_loss: 1.7415 - val_acc: 0.2871\n",
      "Epoch 195/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6448 - acc: 0.3763 - val_loss: 1.7401 - val_acc: 0.2921\n",
      "Epoch 196/300\n",
      "36/36 [==============================] - 0s 940us/step - loss: 1.6438 - acc: 0.3711 - val_loss: 1.7414 - val_acc: 0.2871\n",
      "Epoch 197/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6447 - acc: 0.3763 - val_loss: 1.7402 - val_acc: 0.2871\n",
      "Epoch 198/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6450 - acc: 0.3746 - val_loss: 1.7432 - val_acc: 0.2970\n",
      "Epoch 199/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6430 - acc: 0.3728 - val_loss: 1.7390 - val_acc: 0.2822\n",
      "Epoch 200/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6452 - acc: 0.3763 - val_loss: 1.7413 - val_acc: 0.2921\n",
      "Epoch 201/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6443 - acc: 0.3737 - val_loss: 1.7416 - val_acc: 0.2921\n",
      "Epoch 202/300\n",
      "36/36 [==============================] - 0s 998us/step - loss: 1.6426 - acc: 0.3737 - val_loss: 1.7421 - val_acc: 0.2822\n",
      "Epoch 203/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6441 - acc: 0.3719 - val_loss: 1.7418 - val_acc: 0.2871\n",
      "Epoch 204/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6437 - acc: 0.3772 - val_loss: 1.7430 - val_acc: 0.2822\n",
      "Epoch 205/300\n",
      "36/36 [==============================] - 0s 943us/step - loss: 1.6426 - acc: 0.3754 - val_loss: 1.7424 - val_acc: 0.2970\n",
      "Epoch 206/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6432 - acc: 0.3825 - val_loss: 1.7421 - val_acc: 0.2871\n",
      "Epoch 207/300\n",
      "36/36 [==============================] - 0s 996us/step - loss: 1.6431 - acc: 0.3763 - val_loss: 1.7422 - val_acc: 0.2921\n",
      "Epoch 208/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6427 - acc: 0.3772 - val_loss: 1.7420 - val_acc: 0.2871\n",
      "Epoch 209/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6433 - acc: 0.3763 - val_loss: 1.7414 - val_acc: 0.2871\n",
      "Epoch 210/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6433 - acc: 0.3754 - val_loss: 1.7436 - val_acc: 0.2970\n",
      "Epoch 211/300\n",
      "36/36 [==============================] - 0s 960us/step - loss: 1.6432 - acc: 0.3772 - val_loss: 1.7451 - val_acc: 0.2921\n",
      "Epoch 212/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6420 - acc: 0.3772 - val_loss: 1.7428 - val_acc: 0.2970\n",
      "Epoch 213/300\n",
      "36/36 [==============================] - 0s 974us/step - loss: 1.6431 - acc: 0.3781 - val_loss: 1.7412 - val_acc: 0.2822\n",
      "Epoch 214/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6423 - acc: 0.3781 - val_loss: 1.7434 - val_acc: 0.2921\n",
      "Epoch 215/300\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.8543 - acc: 0.218 - 0s 960us/step - loss: 1.6419 - acc: 0.3798 - val_loss: 1.7430 - val_acc: 0.2822\n",
      "Epoch 216/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6420 - acc: 0.3746 - val_loss: 1.7433 - val_acc: 0.2871\n",
      "Epoch 217/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6429 - acc: 0.3763 - val_loss: 1.7426 - val_acc: 0.2772\n",
      "Epoch 218/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6427 - acc: 0.3754 - val_loss: 1.7441 - val_acc: 0.2871\n",
      "Epoch 219/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6421 - acc: 0.3789 - val_loss: 1.7440 - val_acc: 0.2871\n",
      "Epoch 220/300\n",
      "36/36 [==============================] - 0s 963us/step - loss: 1.6414 - acc: 0.3746 - val_loss: 1.7425 - val_acc: 0.2871\n",
      "Epoch 221/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6428 - acc: 0.3781 - val_loss: 1.7426 - val_acc: 0.2871\n",
      "Epoch 222/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6416 - acc: 0.3763 - val_loss: 1.7464 - val_acc: 0.2970\n",
      "Epoch 223/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6419 - acc: 0.3798 - val_loss: 1.7433 - val_acc: 0.2871\n",
      "Epoch 224/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6415 - acc: 0.3807 - val_loss: 1.7441 - val_acc: 0.3020\n",
      "Epoch 225/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6411 - acc: 0.3789 - val_loss: 1.7437 - val_acc: 0.2871\n",
      "Epoch 226/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6420 - acc: 0.3842 - val_loss: 1.7448 - val_acc: 0.2871\n",
      "Epoch 227/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6413 - acc: 0.3746 - val_loss: 1.7434 - val_acc: 0.2822\n",
      "Epoch 228/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6423 - acc: 0.3798 - val_loss: 1.7435 - val_acc: 0.2921\n",
      "Epoch 229/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6406 - acc: 0.3798 - val_loss: 1.7452 - val_acc: 0.2921\n",
      "Epoch 230/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6414 - acc: 0.3781 - val_loss: 1.7429 - val_acc: 0.2822\n",
      "Epoch 231/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6412 - acc: 0.3789 - val_loss: 1.7461 - val_acc: 0.2921\n",
      "Epoch 232/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6406 - acc: 0.3807 - val_loss: 1.7450 - val_acc: 0.2871\n",
      "Epoch 233/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6407 - acc: 0.3772 - val_loss: 1.7439 - val_acc: 0.2871\n",
      "Epoch 234/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6405 - acc: 0.3825 - val_loss: 1.7449 - val_acc: 0.2921\n",
      "Epoch 235/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6407 - acc: 0.3807 - val_loss: 1.7455 - val_acc: 0.2921\n",
      "Epoch 236/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6415 - acc: 0.3772 - val_loss: 1.7465 - val_acc: 0.2871\n",
      "Epoch 237/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6408 - acc: 0.3816 - val_loss: 1.7461 - val_acc: 0.2921\n",
      "Epoch 238/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6402 - acc: 0.3772 - val_loss: 1.7452 - val_acc: 0.2970\n",
      "Epoch 239/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6409 - acc: 0.3833 - val_loss: 1.7473 - val_acc: 0.2970\n",
      "Epoch 240/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6410 - acc: 0.3772 - val_loss: 1.7448 - val_acc: 0.2970\n",
      "Epoch 241/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6398 - acc: 0.3825 - val_loss: 1.7465 - val_acc: 0.3020\n",
      "Epoch 242/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6403 - acc: 0.3719 - val_loss: 1.7465 - val_acc: 0.2970\n",
      "Epoch 243/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6395 - acc: 0.3833 - val_loss: 1.7466 - val_acc: 0.3020\n",
      "Epoch 244/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6395 - acc: 0.3842 - val_loss: 1.7462 - val_acc: 0.2970\n",
      "Epoch 245/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6394 - acc: 0.3781 - val_loss: 1.7450 - val_acc: 0.2921\n",
      "Epoch 246/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6401 - acc: 0.3746 - val_loss: 1.7471 - val_acc: 0.2921\n",
      "Epoch 247/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6399 - acc: 0.3851 - val_loss: 1.7477 - val_acc: 0.2970\n",
      "Epoch 248/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6397 - acc: 0.3772 - val_loss: 1.7464 - val_acc: 0.2921\n",
      "Epoch 249/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6403 - acc: 0.3807 - val_loss: 1.7469 - val_acc: 0.2921\n",
      "Epoch 250/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6402 - acc: 0.3816 - val_loss: 1.7479 - val_acc: 0.2970\n",
      "Epoch 251/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6392 - acc: 0.3833 - val_loss: 1.7458 - val_acc: 0.2970\n",
      "Epoch 252/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6396 - acc: 0.3746 - val_loss: 1.7469 - val_acc: 0.2970\n",
      "Epoch 253/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6396 - acc: 0.3807 - val_loss: 1.7481 - val_acc: 0.2871\n",
      "Epoch 254/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6398 - acc: 0.3825 - val_loss: 1.7486 - val_acc: 0.2921\n",
      "Epoch 255/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6387 - acc: 0.3807 - val_loss: 1.7488 - val_acc: 0.3020\n",
      "Epoch 256/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6406 - acc: 0.3781 - val_loss: 1.7487 - val_acc: 0.2921\n",
      "Epoch 257/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6394 - acc: 0.3781 - val_loss: 1.7477 - val_acc: 0.2822\n",
      "Epoch 258/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6392 - acc: 0.3807 - val_loss: 1.7471 - val_acc: 0.2871\n",
      "Epoch 259/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6392 - acc: 0.3789 - val_loss: 1.7483 - val_acc: 0.2970\n",
      "Epoch 260/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6390 - acc: 0.3798 - val_loss: 1.7494 - val_acc: 0.2871\n",
      "Epoch 261/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6387 - acc: 0.3833 - val_loss: 1.7488 - val_acc: 0.2921\n",
      "Epoch 262/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6383 - acc: 0.3807 - val_loss: 1.7479 - val_acc: 0.2871\n",
      "Epoch 263/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6384 - acc: 0.3763 - val_loss: 1.7463 - val_acc: 0.2772\n",
      "Epoch 264/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6388 - acc: 0.3825 - val_loss: 1.7498 - val_acc: 0.2970\n",
      "Epoch 265/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6394 - acc: 0.3816 - val_loss: 1.7502 - val_acc: 0.2970\n",
      "Epoch 266/300\n",
      "36/36 [==============================] - 0s 986us/step - loss: 1.6387 - acc: 0.3772 - val_loss: 1.7502 - val_acc: 0.2970\n",
      "Epoch 267/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6392 - acc: 0.3789 - val_loss: 1.7487 - val_acc: 0.2970\n",
      "Epoch 268/300\n",
      "36/36 [==============================] - 0s 967us/step - loss: 1.6382 - acc: 0.3825 - val_loss: 1.7486 - val_acc: 0.2921\n",
      "Epoch 269/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6398 - acc: 0.3886 - val_loss: 1.7522 - val_acc: 0.2921\n",
      "Epoch 270/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6396 - acc: 0.3825 - val_loss: 1.7490 - val_acc: 0.2970\n",
      "Epoch 271/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6391 - acc: 0.3789 - val_loss: 1.7487 - val_acc: 0.2970\n",
      "Epoch 272/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6383 - acc: 0.3851 - val_loss: 1.7501 - val_acc: 0.2871\n",
      "Epoch 273/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6375 - acc: 0.3833 - val_loss: 1.7488 - val_acc: 0.2921\n",
      "Epoch 274/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6380 - acc: 0.3842 - val_loss: 1.7482 - val_acc: 0.2871\n",
      "Epoch 275/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6373 - acc: 0.3789 - val_loss: 1.7491 - val_acc: 0.3020\n",
      "Epoch 276/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6379 - acc: 0.3772 - val_loss: 1.7502 - val_acc: 0.2970\n",
      "Epoch 277/300\n",
      "36/36 [==============================] - 0s 970us/step - loss: 1.6375 - acc: 0.3807 - val_loss: 1.7489 - val_acc: 0.3020\n",
      "Epoch 278/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6383 - acc: 0.3842 - val_loss: 1.7505 - val_acc: 0.2921\n",
      "Epoch 279/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6377 - acc: 0.3825 - val_loss: 1.7513 - val_acc: 0.3020\n",
      "Epoch 280/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6380 - acc: 0.3825 - val_loss: 1.7502 - val_acc: 0.2921\n",
      "Epoch 281/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6379 - acc: 0.3851 - val_loss: 1.7511 - val_acc: 0.2970\n",
      "Epoch 282/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6371 - acc: 0.3842 - val_loss: 1.7498 - val_acc: 0.2921\n",
      "Epoch 283/300\n",
      "36/36 [==============================] - 0s 969us/step - loss: 1.6369 - acc: 0.3816 - val_loss: 1.7506 - val_acc: 0.2921\n",
      "Epoch 284/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6372 - acc: 0.3851 - val_loss: 1.7509 - val_acc: 0.2871\n",
      "Epoch 285/300\n",
      "36/36 [==============================] - 0s 970us/step - loss: 1.6371 - acc: 0.3842 - val_loss: 1.7506 - val_acc: 0.2921\n",
      "Epoch 286/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6374 - acc: 0.3868 - val_loss: 1.7495 - val_acc: 0.2723\n",
      "Epoch 287/300\n",
      "36/36 [==============================] - 0s 972us/step - loss: 1.6377 - acc: 0.3807 - val_loss: 1.7509 - val_acc: 0.2871\n",
      "Epoch 288/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6378 - acc: 0.3798 - val_loss: 1.7543 - val_acc: 0.2871\n",
      "Epoch 289/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6366 - acc: 0.3851 - val_loss: 1.7512 - val_acc: 0.2921\n",
      "Epoch 290/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6368 - acc: 0.3842 - val_loss: 1.7498 - val_acc: 0.2822\n",
      "Epoch 291/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6378 - acc: 0.3851 - val_loss: 1.7516 - val_acc: 0.2970\n",
      "Epoch 292/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6365 - acc: 0.3851 - val_loss: 1.7497 - val_acc: 0.2822\n",
      "Epoch 293/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6371 - acc: 0.3921 - val_loss: 1.7529 - val_acc: 0.2921\n",
      "Epoch 294/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6367 - acc: 0.3833 - val_loss: 1.7506 - val_acc: 0.2871\n",
      "Epoch 295/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6363 - acc: 0.3842 - val_loss: 1.7521 - val_acc: 0.2822\n",
      "Epoch 296/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6366 - acc: 0.3816 - val_loss: 1.7524 - val_acc: 0.2871\n",
      "Epoch 297/300\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6366 - acc: 0.3833 - val_loss: 1.7510 - val_acc: 0.2871\n",
      "Epoch 298/300\n",
      "36/36 [==============================] - 0s 986us/step - loss: 1.6362 - acc: 0.3842 - val_loss: 1.7508 - val_acc: 0.2822\n",
      "Epoch 299/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6367 - acc: 0.3842 - val_loss: 1.7523 - val_acc: 0.3020\n",
      "Epoch 300/300\n",
      "36/36 [==============================] - 0s 997us/step - loss: 1.6374 - acc: 0.3816 - val_loss: 1.7531 - val_acc: 0.2970\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data.paper_id.to_numpy()\n",
    "print(x_train.shape)\n",
    "history = run_experiment(gnn_model.get_model(), x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2708, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, array([[  21,  905,  906, ..., 2586, 1874, 2707],\n",
      "       [   0,    0,    0, ..., 1874, 1876, 1897]], dtype=int64), <tf.Tensor: shape=(5429,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)>)\n",
      "7\n",
      "GNN output shape: tf.Tensor(\n",
      "[[ 0.07014371 -0.08579759  0.11451342  0.02328166  0.07051581  0.22653823\n",
      "   0.14369097]\n",
      " [ 0.11443309  0.00081104  0.03951072  0.05815473  0.01158473  0.04589193\n",
      "  -0.27310726]\n",
      " [-0.05110506 -0.00064083  0.00840581 -0.027003    0.08705085  0.06186487\n",
      "   0.02247791]], shape=(3, 7), dtype=float32)\n",
      "Model: \"gnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " preprocess (Sequential)     (2708, 32)                52804     \n",
      "                                                                 \n",
      " graph_conv1 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " graph_conv2 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " postprocess (Sequential)    (2708, 32)                2368      \n",
      "                                                                 \n",
      " logits (Dense)              multiple                  231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,179\n",
      "Trainable params: 63,481\n",
      "Non-trainable params: 3,698\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "print(graph_info)\n",
    "print(num_classes)\n",
    "\n",
    "gnn_model = GNNNodeClassifier(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiment(model, x_train, y_train):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1342, 1433), (1342,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 3s 169ms/step - loss: 2.2768 - acc: 0.1575 - val_loss: 1.9281 - val_acc: 0.1232\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.9714 - acc: 0.2576 - val_loss: 1.9221 - val_acc: 0.1429\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.9510 - acc: 0.2689 - val_loss: 1.9142 - val_acc: 0.2020\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.8722 - acc: 0.2785 - val_loss: 1.9089 - val_acc: 0.1576\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.8632 - acc: 0.2846 - val_loss: 1.9113 - val_acc: 0.1330\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.8595 - acc: 0.2750 - val_loss: 1.9018 - val_acc: 0.1675\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.8633 - acc: 0.2950 - val_loss: 1.8809 - val_acc: 0.3054\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 1.8209 - acc: 0.3046 - val_loss: 1.8585 - val_acc: 0.3448\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.7892 - acc: 0.3133 - val_loss: 1.8279 - val_acc: 0.3695\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.7702 - acc: 0.3124 - val_loss: 1.7887 - val_acc: 0.3941\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.7555 - acc: 0.3142 - val_loss: 1.7352 - val_acc: 0.4483\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.7300 - acc: 0.3473 - val_loss: 1.6717 - val_acc: 0.4975\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.6945 - acc: 0.3490 - val_loss: 1.5881 - val_acc: 0.5074\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.6566 - acc: 0.3899 - val_loss: 1.5019 - val_acc: 0.5123\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.6287 - acc: 0.3716 - val_loss: 1.3912 - val_acc: 0.5074\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.5589 - acc: 0.3995 - val_loss: 1.3357 - val_acc: 0.5025\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1.5069 - acc: 0.4212 - val_loss: 1.2943 - val_acc: 0.5123\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.5016 - acc: 0.4317 - val_loss: 1.3101 - val_acc: 0.5222\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.4604 - acc: 0.4413 - val_loss: 1.4056 - val_acc: 0.5320\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.3843 - acc: 0.4752 - val_loss: 1.5204 - val_acc: 0.5123\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 1.3372 - acc: 0.4961 - val_loss: 1.5019 - val_acc: 0.5025\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.3425 - acc: 0.4830 - val_loss: 1.5928 - val_acc: 0.5123\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 1.3019 - acc: 0.5213 - val_loss: 1.5283 - val_acc: 0.5567\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.2365 - acc: 0.5292 - val_loss: 1.3960 - val_acc: 0.6010\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.2464 - acc: 0.5309 - val_loss: 1.2110 - val_acc: 0.6108\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.2076 - acc: 0.5527 - val_loss: 1.1626 - val_acc: 0.6305\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.1399 - acc: 0.5970 - val_loss: 1.1688 - val_acc: 0.6010\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.1685 - acc: 0.5779 - val_loss: 1.0993 - val_acc: 0.6059\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.1371 - acc: 0.5788 - val_loss: 0.9883 - val_acc: 0.6847\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.1024 - acc: 0.6023 - val_loss: 1.0244 - val_acc: 0.6700\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.0476 - acc: 0.6214 - val_loss: 1.0325 - val_acc: 0.6700\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.0584 - acc: 0.5997 - val_loss: 1.1133 - val_acc: 0.6552\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.0242 - acc: 0.6327 - val_loss: 1.1125 - val_acc: 0.6650\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.0267 - acc: 0.6258 - val_loss: 1.1525 - val_acc: 0.5911\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.9586 - acc: 0.6562 - val_loss: 1.1389 - val_acc: 0.5862\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.9609 - acc: 0.6701 - val_loss: 1.0858 - val_acc: 0.6305\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.9466 - acc: 0.6780 - val_loss: 0.8962 - val_acc: 0.6995\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.9462 - acc: 0.6867 - val_loss: 0.8638 - val_acc: 0.7340\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.9422 - acc: 0.6728 - val_loss: 0.8345 - val_acc: 0.7340\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.8779 - acc: 0.7058 - val_loss: 0.8247 - val_acc: 0.7192\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.8460 - acc: 0.7015 - val_loss: 0.8423 - val_acc: 0.7044\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.9302 - acc: 0.6902 - val_loss: 0.8754 - val_acc: 0.6897\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.8607 - acc: 0.6963 - val_loss: 0.8343 - val_acc: 0.7094\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.8286 - acc: 0.7171 - val_loss: 0.8074 - val_acc: 0.7192\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7728 - acc: 0.7406 - val_loss: 0.7959 - val_acc: 0.7340\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7889 - acc: 0.7311 - val_loss: 0.7499 - val_acc: 0.7685\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.8037 - acc: 0.7215 - val_loss: 0.8234 - val_acc: 0.7438\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7898 - acc: 0.7450 - val_loss: 0.9253 - val_acc: 0.7143\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.7759 - acc: 0.7372 - val_loss: 0.9566 - val_acc: 0.6946\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.7546 - acc: 0.7380 - val_loss: 0.9141 - val_acc: 0.6995\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.7777 - acc: 0.7337 - val_loss: 0.9343 - val_acc: 0.7044\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7186 - acc: 0.7607 - val_loss: 0.9283 - val_acc: 0.6995\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.6863 - acc: 0.7537 - val_loss: 0.8745 - val_acc: 0.7340\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.7424 - acc: 0.7537 - val_loss: 0.7779 - val_acc: 0.7291\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.7085 - acc: 0.7824 - val_loss: 0.7094 - val_acc: 0.7635\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.7209 - acc: 0.7598 - val_loss: 0.6850 - val_acc: 0.7685\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7099 - acc: 0.7720 - val_loss: 0.6861 - val_acc: 0.7586\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6763 - acc: 0.7641 - val_loss: 0.6603 - val_acc: 0.7586\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.7017 - acc: 0.7563 - val_loss: 0.6820 - val_acc: 0.7537\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7074 - acc: 0.7389 - val_loss: 0.8150 - val_acc: 0.7044\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.6508 - acc: 0.7755 - val_loss: 0.7037 - val_acc: 0.7537\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.6705 - acc: 0.7720 - val_loss: 0.6180 - val_acc: 0.7783\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.6179 - acc: 0.8050 - val_loss: 0.6169 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.6222 - acc: 0.7833 - val_loss: 0.6087 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6181 - acc: 0.7772 - val_loss: 0.6045 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6009 - acc: 0.8033 - val_loss: 0.5947 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6353 - acc: 0.7894 - val_loss: 0.5977 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.6138 - acc: 0.7998 - val_loss: 0.5893 - val_acc: 0.8227\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.5942 - acc: 0.8016 - val_loss: 0.6050 - val_acc: 0.8128\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5923 - acc: 0.8094 - val_loss: 0.6496 - val_acc: 0.7931\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6446 - acc: 0.7850 - val_loss: 0.5768 - val_acc: 0.7980\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6172 - acc: 0.8024 - val_loss: 0.6034 - val_acc: 0.7882\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6190 - acc: 0.7911 - val_loss: 0.5778 - val_acc: 0.7931\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.5884 - acc: 0.7981 - val_loss: 0.6036 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6240 - acc: 0.7929 - val_loss: 0.6290 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5946 - acc: 0.8042 - val_loss: 0.6100 - val_acc: 0.7833\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5932 - acc: 0.7955 - val_loss: 0.5788 - val_acc: 0.7980\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5631 - acc: 0.8120 - val_loss: 0.5879 - val_acc: 0.8030\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.5681 - acc: 0.8216 - val_loss: 0.6227 - val_acc: 0.7931\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5879 - acc: 0.8077 - val_loss: 0.7250 - val_acc: 0.7488\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.6146 - acc: 0.7998 - val_loss: 0.7265 - val_acc: 0.7635\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5065 - acc: 0.8346 - val_loss: 0.6595 - val_acc: 0.7734\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5461 - acc: 0.8138 - val_loss: 0.6444 - val_acc: 0.7635\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.5304 - acc: 0.8372 - val_loss: 0.6075 - val_acc: 0.7783\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5454 - acc: 0.8277 - val_loss: 0.5609 - val_acc: 0.7980\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5366 - acc: 0.8251 - val_loss: 0.5320 - val_acc: 0.7980\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5639 - acc: 0.8138 - val_loss: 0.5272 - val_acc: 0.7931\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.5488 - acc: 0.8111 - val_loss: 0.5915 - val_acc: 0.7980\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5605 - acc: 0.8050 - val_loss: 0.5954 - val_acc: 0.7882\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5201 - acc: 0.8251 - val_loss: 0.5401 - val_acc: 0.8227\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5074 - acc: 0.8477 - val_loss: 0.5197 - val_acc: 0.8424\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5317 - acc: 0.8320 - val_loss: 0.5153 - val_acc: 0.8276\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4610 - acc: 0.8538 - val_loss: 0.5011 - val_acc: 0.8276\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5121 - acc: 0.8277 - val_loss: 0.4985 - val_acc: 0.8276\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4894 - acc: 0.8468 - val_loss: 0.4887 - val_acc: 0.8473\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5047 - acc: 0.8460 - val_loss: 0.4899 - val_acc: 0.8325\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.4642 - acc: 0.8468 - val_loss: 0.5470 - val_acc: 0.7931\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4986 - acc: 0.8390 - val_loss: 0.5111 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4681 - acc: 0.8529 - val_loss: 0.4849 - val_acc: 0.8473\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.5199 - acc: 0.8329 - val_loss: 0.4739 - val_acc: 0.8473\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4905 - acc: 0.8364 - val_loss: 0.5206 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4943 - acc: 0.8425 - val_loss: 0.5311 - val_acc: 0.8227\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4645 - acc: 0.8477 - val_loss: 0.5008 - val_acc: 0.8325\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4805 - acc: 0.8390 - val_loss: 0.4876 - val_acc: 0.8128\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4845 - acc: 0.8477 - val_loss: 0.5243 - val_acc: 0.8128\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4836 - acc: 0.8460 - val_loss: 0.5081 - val_acc: 0.8227\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4945 - acc: 0.8407 - val_loss: 0.4865 - val_acc: 0.8177\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.4769 - acc: 0.8503 - val_loss: 0.4907 - val_acc: 0.8227\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4829 - acc: 0.8529 - val_loss: 0.5065 - val_acc: 0.8177\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4752 - acc: 0.8512 - val_loss: 0.5507 - val_acc: 0.8030\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4535 - acc: 0.8564 - val_loss: 0.5359 - val_acc: 0.8128\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4766 - acc: 0.8468 - val_loss: 0.4942 - val_acc: 0.8276\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4867 - acc: 0.8390 - val_loss: 0.4886 - val_acc: 0.8374\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4571 - acc: 0.8547 - val_loss: 0.4894 - val_acc: 0.8374\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4591 - acc: 0.8433 - val_loss: 0.4756 - val_acc: 0.8374\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4413 - acc: 0.8634 - val_loss: 0.4715 - val_acc: 0.8424\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4608 - acc: 0.8416 - val_loss: 0.4805 - val_acc: 0.8374\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4612 - acc: 0.8677 - val_loss: 0.4469 - val_acc: 0.8473\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4504 - acc: 0.8607 - val_loss: 0.4498 - val_acc: 0.8818\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4931 - acc: 0.8277 - val_loss: 0.4564 - val_acc: 0.8621\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4410 - acc: 0.8599 - val_loss: 0.4835 - val_acc: 0.8424\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4343 - acc: 0.8634 - val_loss: 0.4676 - val_acc: 0.8473\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4283 - acc: 0.8547 - val_loss: 0.4479 - val_acc: 0.8571\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4054 - acc: 0.8668 - val_loss: 0.4226 - val_acc: 0.8670\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4344 - acc: 0.8564 - val_loss: 0.4155 - val_acc: 0.8719\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4261 - acc: 0.8703 - val_loss: 0.4181 - val_acc: 0.8818\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4567 - acc: 0.8581 - val_loss: 0.4182 - val_acc: 0.8719\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4268 - acc: 0.8599 - val_loss: 0.4124 - val_acc: 0.8719\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4396 - acc: 0.8599 - val_loss: 0.4460 - val_acc: 0.8522\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3933 - acc: 0.8764 - val_loss: 0.4521 - val_acc: 0.8522\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.4395 - acc: 0.8747 - val_loss: 0.4605 - val_acc: 0.8522\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4132 - acc: 0.8712 - val_loss: 0.4801 - val_acc: 0.8473\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4596 - acc: 0.8686 - val_loss: 0.4952 - val_acc: 0.8424\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3682 - acc: 0.8755 - val_loss: 0.4553 - val_acc: 0.8522\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4463 - acc: 0.8555 - val_loss: 0.4202 - val_acc: 0.8571\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4490 - acc: 0.8616 - val_loss: 0.4123 - val_acc: 0.8670\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4073 - acc: 0.8782 - val_loss: 0.4349 - val_acc: 0.8768\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4276 - acc: 0.8581 - val_loss: 0.4375 - val_acc: 0.8670\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4002 - acc: 0.8695 - val_loss: 0.4195 - val_acc: 0.8719\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4305 - acc: 0.8668 - val_loss: 0.4322 - val_acc: 0.8867\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4124 - acc: 0.8755 - val_loss: 0.4813 - val_acc: 0.8522\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4328 - acc: 0.8599 - val_loss: 0.5128 - val_acc: 0.8522\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4033 - acc: 0.8782 - val_loss: 0.4990 - val_acc: 0.8571\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3996 - acc: 0.8799 - val_loss: 0.4843 - val_acc: 0.8670\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.4049 - acc: 0.8764 - val_loss: 0.4792 - val_acc: 0.8571\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3669 - acc: 0.8721 - val_loss: 0.4886 - val_acc: 0.8473\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4239 - acc: 0.8703 - val_loss: 0.4851 - val_acc: 0.8522\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4229 - acc: 0.8712 - val_loss: 0.4488 - val_acc: 0.8719\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4030 - acc: 0.8808 - val_loss: 0.4259 - val_acc: 0.8719\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3495 - acc: 0.8877 - val_loss: 0.4303 - val_acc: 0.8719\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3920 - acc: 0.8677 - val_loss: 0.4509 - val_acc: 0.8571\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.3964 - acc: 0.8721 - val_loss: 0.4770 - val_acc: 0.8325\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3700 - acc: 0.8790 - val_loss: 0.4608 - val_acc: 0.8473\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4146 - acc: 0.8747 - val_loss: 0.4563 - val_acc: 0.8473\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3656 - acc: 0.8851 - val_loss: 0.4633 - val_acc: 0.8424\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4351 - acc: 0.8729 - val_loss: 0.4666 - val_acc: 0.8522\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.3835 - acc: 0.8799 - val_loss: 0.4705 - val_acc: 0.8473\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.3929 - acc: 0.8764 - val_loss: 0.4893 - val_acc: 0.8276\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4144 - acc: 0.8660 - val_loss: 0.5160 - val_acc: 0.8374\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3975 - acc: 0.8712 - val_loss: 0.5426 - val_acc: 0.8325\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3684 - acc: 0.8930 - val_loss: 0.4897 - val_acc: 0.8276\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3660 - acc: 0.8834 - val_loss: 0.4569 - val_acc: 0.8621\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4077 - acc: 0.8642 - val_loss: 0.4404 - val_acc: 0.8768\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3745 - acc: 0.8877 - val_loss: 0.4556 - val_acc: 0.8621\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3897 - acc: 0.8695 - val_loss: 0.5059 - val_acc: 0.8325\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3597 - acc: 0.8860 - val_loss: 0.4770 - val_acc: 0.8374\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3867 - acc: 0.8703 - val_loss: 0.4582 - val_acc: 0.8522\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4303 - acc: 0.8642 - val_loss: 0.4581 - val_acc: 0.8522\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3512 - acc: 0.8903 - val_loss: 0.4785 - val_acc: 0.8571\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3810 - acc: 0.8773 - val_loss: 0.4871 - val_acc: 0.8424\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4062 - acc: 0.8695 - val_loss: 0.4871 - val_acc: 0.8473\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3768 - acc: 0.8842 - val_loss: 0.5106 - val_acc: 0.8473\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3572 - acc: 0.8930 - val_loss: 0.5345 - val_acc: 0.8424\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4093 - acc: 0.8738 - val_loss: 0.5260 - val_acc: 0.8276\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3950 - acc: 0.8677 - val_loss: 0.4876 - val_acc: 0.8424\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3483 - acc: 0.8930 - val_loss: 0.4446 - val_acc: 0.8571\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3605 - acc: 0.8782 - val_loss: 0.4700 - val_acc: 0.8424\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3549 - acc: 0.8964 - val_loss: 0.4603 - val_acc: 0.8424\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3934 - acc: 0.8747 - val_loss: 0.4540 - val_acc: 0.8621\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.4026 - acc: 0.8851 - val_loss: 0.4614 - val_acc: 0.8719\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3465 - acc: 0.8869 - val_loss: 0.4787 - val_acc: 0.8571\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3832 - acc: 0.8808 - val_loss: 0.5070 - val_acc: 0.8374\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3253 - acc: 0.8912 - val_loss: 0.4619 - val_acc: 0.8424\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3325 - acc: 0.9008 - val_loss: 0.4429 - val_acc: 0.8522\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3897 - acc: 0.8729 - val_loss: 0.4431 - val_acc: 0.8522\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3456 - acc: 0.8956 - val_loss: 0.4602 - val_acc: 0.8424\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3702 - acc: 0.8921 - val_loss: 0.4619 - val_acc: 0.8424\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3894 - acc: 0.8947 - val_loss: 0.4486 - val_acc: 0.8473\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3577 - acc: 0.8869 - val_loss: 0.4729 - val_acc: 0.8473\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3489 - acc: 0.8877 - val_loss: 0.4936 - val_acc: 0.8473\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data.paper_id.to_numpy()\n",
    "history = run_experiment(gnn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 30.53%\n"
     ]
    }
   ],
   "source": [
    "x_test = test_data.paper_id.to_numpy()\n",
    "_, test_accuracy = gnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = loader.class_values\n",
    "node_features=loader.node_features\n",
    "edges = loader.edges\n",
    "\n",
    "def display_class_probabilities(probabilities):\n",
    "    for instance_idx, probs in enumerate(probabilities):\n",
    "        print(f\"Instance {instance_idx + 1}:\")\n",
    "        for class_idx, prob in enumerate(probs):\n",
    "            print(f\"- {class_values[class_idx]}: {round(prob * 100, 2)}%\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GNNModel' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5500/1116990751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2708\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2709\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2710\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices[:2]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay_class_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GNNModel' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "logits = gnn_model.predict(tf.constant([2708, 2709, 2710]))\n",
    "# logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices[:2]))\n",
    "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
    "display_class_probabilities(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5812e37fbc42ae0019c075dcd625ea6adf837b197758e07cfdfe5b415c77a600"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
