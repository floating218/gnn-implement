{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='conv.png' width=80%>\n",
    "<img src='gnn.png' width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_dataset(self):    \n",
    "        \n",
    "        '''\n",
    "        citations: [target논문인덱스, source논문인덱스]\n",
    "        papers: [논문인덱스, 1424개 단어 포함 여부, 주제(subject)]\n",
    "        train_data: papers 데이터 중 50% 샘플링\n",
    "        test_data: papers 데이터 중 50% 샘플링\n",
    "        x_train: train_data 중, 논문인덱스와 subject를 제외한 피쳐\n",
    "        y_train: train_data 중 subject에 해당하는 레이블\n",
    "        '''\n",
    "        \n",
    "        zip_file = keras.utils.get_file(\n",
    "            fname=\"cora.tgz\",\n",
    "            origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "            extract=True,\n",
    "        )\n",
    "        data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "        \n",
    "        citations = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.cites\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"target\", \"source\"],\n",
    "        )\n",
    "        \n",
    "        column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "        papers = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.content\"), \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=column_names,\n",
    "        )\n",
    "        \n",
    "        class_values = sorted(papers[\"subject\"].unique())\n",
    "        class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "        paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "        papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "        papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "        \n",
    "        train_data, test_data = [], []\n",
    "\n",
    "        for _, group_data in papers.groupby(\"subject\"):\n",
    "            # Select around 50% of the dataset for training.\n",
    "            random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "            train_data.append(group_data[random_selection])\n",
    "            test_data.append(group_data[~random_selection])\n",
    "\n",
    "        train_data = pd.concat(train_data).sample(frac=1)\n",
    "        test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "        print(\"citations data shape:\", citations.shape)\n",
    "        print(\"papers data shape:\", papers.shape)\n",
    "        print(\"Train data shape:\", train_data.shape)\n",
    "        print(\"Test data shape:\", test_data.shape)\n",
    "        \n",
    "        feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
    "        num_features = len(feature_names)\n",
    "        num_classes = len(class_idx)\n",
    "\n",
    "        # Create train and test features as a numpy array.\n",
    "        x_train = train_data[feature_names].to_numpy()\n",
    "        x_test = test_data[feature_names].to_numpy()\n",
    "        \n",
    "        # Create train and test targets as a numpy array.\n",
    "        y_train = train_data[\"subject\"]\n",
    "        y_test = test_data[\"subject\"]\n",
    "        \n",
    "        # Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
    "        edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "        # Create an edge weights array of ones.\n",
    "        edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Create a node features array of shape [num_nodes, num_features].\n",
    "        node_features = tf.cast(\n",
    "            papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    "        )\n",
    "        # Create graph info tuple with node_features, edges, and edge_weights.\n",
    "        graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "        print(\"Edges shape:\", edges.shape)\n",
    "        print(\"Nodes shape:\", node_features.shape)\n",
    "        \n",
    "        self.feature_names = feature_names #단어 피쳐 컬럼 이름\n",
    "        self.num_features = num_features #단어 피쳐 개수\n",
    "        self.num_classes = num_classes #주제subject의 가짓수\n",
    "        self.graph_info = graph_info\n",
    "        self.node_features = node_features #논문 인덱스 순으로 정리된 상태의 feature 매트릭스\n",
    "        self.edges = edges\n",
    "        self.class_values = class_values\n",
    "        \n",
    "        return citations, papers, x_train, x_test, y_train, y_test, train_data, test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citations data shape: (5429, 2)\n",
      "papers data shape: (2708, 1435)\n",
      "Train data shape: (1378, 1435)\n",
      "Test data shape: (1330, 1435)\n",
      "Edges shape: (2, 5429)\n",
      "Nodes shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "loader = Loader()\n",
    "citations, papers, x_train, x_test, y_train, y_test, train_data, test_data = loader.load_dataset()\n",
    "feature_names = loader.feature_names\n",
    "num_classes = loader.num_classes\n",
    "graph_info = loader.graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    '''\n",
    "hidden_units = [32, 32]\n",
    "dropout_rate = 0.2\n",
    "'''\n",
    "\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN output shape: tf.Tensor(\n",
      "[[ 0.01042217  0.1271193  -0.04811429  0.10840625  0.08604226 -0.10460234\n",
      "   0.04044523]\n",
      " [ 0.10392918 -0.06908441 -0.06521179  0.05655614  0.02580935 -0.09190933\n",
      "  -0.04335675]\n",
      " [ 0.12119567 -0.09860111 -0.04154585  0.08767194 -0.0387424   0.04388608\n",
      "   0.01892218]], shape=(3, 7), dtype=float32)\n",
      "Model: \"gnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " preprocess (Sequential)     (2708, 32)                52804     \n",
      "                                                                 \n",
      " graph_conv1 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " graph_conv2 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " postprocess (Sequential)    (2708, 32)                2368      \n",
      "                                                                 \n",
      " logits (Dense)              multiple                  231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,179\n",
      "Trainable params: 63,481\n",
      "Non-trainable params: 3,698\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "gnn_model = GNNNodeClassifier(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiment(model, x_train, y_train):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 3s 173ms/step - loss: 2.1804 - acc: 0.1716 - val_loss: 1.9168 - val_acc: 0.3043\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.9494 - acc: 0.2878 - val_loss: 1.9069 - val_acc: 0.1643\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.9067 - acc: 0.2886 - val_loss: 1.8975 - val_acc: 0.1787\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.8641 - acc: 0.2972 - val_loss: 1.8819 - val_acc: 0.3816\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.8383 - acc: 0.3091 - val_loss: 1.8681 - val_acc: 0.3430\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.8254 - acc: 0.3057 - val_loss: 1.8437 - val_acc: 0.3188\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.8126 - acc: 0.3228 - val_loss: 1.8179 - val_acc: 0.3333\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.7822 - acc: 0.3271 - val_loss: 1.7967 - val_acc: 0.3575\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.7756 - acc: 0.3382 - val_loss: 1.7707 - val_acc: 0.3816\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 1.7146 - acc: 0.3664 - val_loss: 1.7179 - val_acc: 0.4251\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 1.7149 - acc: 0.3698 - val_loss: 1.6348 - val_acc: 0.4348\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.6708 - acc: 0.3945 - val_loss: 1.5654 - val_acc: 0.4396\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.6315 - acc: 0.3911 - val_loss: 1.4937 - val_acc: 0.4348\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.5924 - acc: 0.4125 - val_loss: 1.4354 - val_acc: 0.4396\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.5637 - acc: 0.4159 - val_loss: 1.3891 - val_acc: 0.4541\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.4909 - acc: 0.4364 - val_loss: 1.4402 - val_acc: 0.4348\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.4990 - acc: 0.4432 - val_loss: 1.6439 - val_acc: 0.4444\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.4526 - acc: 0.4535 - val_loss: 1.5996 - val_acc: 0.4589\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.4253 - acc: 0.4740 - val_loss: 1.3483 - val_acc: 0.4976\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.3627 - acc: 0.4748 - val_loss: 1.2614 - val_acc: 0.4976\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.3167 - acc: 0.5038 - val_loss: 1.2556 - val_acc: 0.4879\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.3185 - acc: 0.5030 - val_loss: 1.2293 - val_acc: 0.5266\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 1.2897 - acc: 0.5158 - val_loss: 1.1912 - val_acc: 0.5556\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.2698 - acc: 0.5235 - val_loss: 1.1589 - val_acc: 0.5652\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.2024 - acc: 0.5568 - val_loss: 1.1421 - val_acc: 0.5507\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.1994 - acc: 0.5491 - val_loss: 1.1222 - val_acc: 0.5652\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.1498 - acc: 0.5790 - val_loss: 1.0970 - val_acc: 0.5652\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.1467 - acc: 0.5713 - val_loss: 1.2844 - val_acc: 0.5556\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.1009 - acc: 0.6003 - val_loss: 1.3137 - val_acc: 0.5700\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.0232 - acc: 0.6183 - val_loss: 1.4605 - val_acc: 0.5314\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.9905 - acc: 0.6456 - val_loss: 1.7938 - val_acc: 0.5169\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.0255 - acc: 0.6413 - val_loss: 1.6151 - val_acc: 0.5362\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.9778 - acc: 0.6430 - val_loss: 1.5722 - val_acc: 0.5749\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.0174 - acc: 0.6516 - val_loss: 1.3428 - val_acc: 0.6039\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.9313 - acc: 0.6678 - val_loss: 1.2168 - val_acc: 0.6377\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.9371 - acc: 0.6840 - val_loss: 1.1606 - val_acc: 0.6377\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.9412 - acc: 0.6695 - val_loss: 1.1112 - val_acc: 0.6329\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.9026 - acc: 0.6883 - val_loss: 1.1211 - val_acc: 0.6280\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.8638 - acc: 0.7003 - val_loss: 1.1586 - val_acc: 0.6087\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.8230 - acc: 0.7199 - val_loss: 1.1776 - val_acc: 0.6087\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.8432 - acc: 0.6977 - val_loss: 1.1241 - val_acc: 0.6087\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.8313 - acc: 0.7071 - val_loss: 1.1476 - val_acc: 0.6280\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7950 - acc: 0.7336 - val_loss: 1.1479 - val_acc: 0.6184\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7632 - acc: 0.7378 - val_loss: 1.1425 - val_acc: 0.6377\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7623 - acc: 0.7395 - val_loss: 1.0469 - val_acc: 0.6329\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.7252 - acc: 0.7600 - val_loss: 1.0501 - val_acc: 0.6329\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7347 - acc: 0.7506 - val_loss: 1.0796 - val_acc: 0.6570\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7404 - acc: 0.7464 - val_loss: 1.0217 - val_acc: 0.6618\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.7136 - acc: 0.7600 - val_loss: 1.0017 - val_acc: 0.6618\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.6616 - acc: 0.7848 - val_loss: 1.0196 - val_acc: 0.6812\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6933 - acc: 0.7669 - val_loss: 1.0168 - val_acc: 0.6860\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.6396 - acc: 0.7720 - val_loss: 0.9991 - val_acc: 0.6812\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7197 - acc: 0.7652 - val_loss: 0.9890 - val_acc: 0.6667\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.7005 - acc: 0.7720 - val_loss: 1.0624 - val_acc: 0.6618\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6850 - acc: 0.7703 - val_loss: 1.2123 - val_acc: 0.6280\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6961 - acc: 0.7583 - val_loss: 1.2323 - val_acc: 0.6329\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6545 - acc: 0.7814 - val_loss: 0.9907 - val_acc: 0.6618\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.6462 - acc: 0.8044 - val_loss: 0.9096 - val_acc: 0.6715\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6453 - acc: 0.7899 - val_loss: 0.8941 - val_acc: 0.6908\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.5817 - acc: 0.8053 - val_loss: 0.9116 - val_acc: 0.7053\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.6509 - acc: 0.7925 - val_loss: 0.8167 - val_acc: 0.7198\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6445 - acc: 0.7865 - val_loss: 0.7649 - val_acc: 0.7295\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6280 - acc: 0.7942 - val_loss: 0.8895 - val_acc: 0.6908\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5567 - acc: 0.8198 - val_loss: 0.8867 - val_acc: 0.6957\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.5843 - acc: 0.8079 - val_loss: 0.8149 - val_acc: 0.7246\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.6019 - acc: 0.8121 - val_loss: 0.9370 - val_acc: 0.6860\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.6241 - acc: 0.8019 - val_loss: 0.9924 - val_acc: 0.6715\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.6252 - acc: 0.8010 - val_loss: 0.8531 - val_acc: 0.7101\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5498 - acc: 0.8155 - val_loss: 0.8576 - val_acc: 0.7246\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5800 - acc: 0.8061 - val_loss: 0.8594 - val_acc: 0.7246\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5394 - acc: 0.8275 - val_loss: 0.8732 - val_acc: 0.7343\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.5218 - acc: 0.8215 - val_loss: 0.8009 - val_acc: 0.7585\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5515 - acc: 0.8232 - val_loss: 0.7442 - val_acc: 0.7488\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5382 - acc: 0.8318 - val_loss: 0.7495 - val_acc: 0.7488\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5734 - acc: 0.8138 - val_loss: 0.7490 - val_acc: 0.7391\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.5373 - acc: 0.8301 - val_loss: 0.7282 - val_acc: 0.7391\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.5585 - acc: 0.8198 - val_loss: 0.6953 - val_acc: 0.7778\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5292 - acc: 0.8335 - val_loss: 0.7134 - val_acc: 0.7536\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.5338 - acc: 0.8232 - val_loss: 0.7170 - val_acc: 0.7488\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.5060 - acc: 0.8301 - val_loss: 0.6864 - val_acc: 0.7536\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4682 - acc: 0.8454 - val_loss: 0.6839 - val_acc: 0.7536\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4858 - acc: 0.8429 - val_loss: 0.7538 - val_acc: 0.7488\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4861 - acc: 0.8377 - val_loss: 0.7215 - val_acc: 0.7729\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5019 - acc: 0.8335 - val_loss: 0.7503 - val_acc: 0.7585\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5037 - acc: 0.8275 - val_loss: 0.7196 - val_acc: 0.7681\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4890 - acc: 0.8506 - val_loss: 0.6879 - val_acc: 0.7729\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.5213 - acc: 0.8326 - val_loss: 0.6772 - val_acc: 0.7826\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4802 - acc: 0.8403 - val_loss: 0.7040 - val_acc: 0.7633\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4872 - acc: 0.8266 - val_loss: 0.6827 - val_acc: 0.7778\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4889 - acc: 0.8352 - val_loss: 0.6904 - val_acc: 0.7826\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4280 - acc: 0.8582 - val_loss: 0.7157 - val_acc: 0.7826\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4336 - acc: 0.8582 - val_loss: 0.6939 - val_acc: 0.7923\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4788 - acc: 0.8446 - val_loss: 0.7154 - val_acc: 0.7585\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4874 - acc: 0.8514 - val_loss: 0.6957 - val_acc: 0.7826\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4784 - acc: 0.8437 - val_loss: 0.6493 - val_acc: 0.7971\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4621 - acc: 0.8617 - val_loss: 0.6722 - val_acc: 0.7778\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4733 - acc: 0.8420 - val_loss: 0.8311 - val_acc: 0.7391\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4950 - acc: 0.8335 - val_loss: 0.7125 - val_acc: 0.7729\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4352 - acc: 0.8514 - val_loss: 0.7033 - val_acc: 0.7778\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.4732 - acc: 0.8463 - val_loss: 0.6864 - val_acc: 0.7874\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data.paper_id.to_numpy()\n",
    "history = run_experiment(gnn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GNNNodeClassifier at 0x239eb091dc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239    2408\n",
       "551      706\n",
       "250      509\n",
       "139     1642\n",
       "417     1230\n",
       "1146     821\n",
       "836     2600\n",
       "64      1374\n",
       "974      877\n",
       "2601    1982\n",
       "Name: paper_id, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.paper_id.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.344487  , -1.9264449 ,  7.687362  ,  0.74706805,\n",
       "         -2.191979  , -1.5481755 , -0.07419495]],\n",
       "\n",
       "       [[ 0.22949243, -1.526889  ,  0.50571024, -0.83466494,\n",
       "         -0.26296428,  2.1523473 ,  0.433302  ]],\n",
       "\n",
       "       [[-3.7569265 , -4.457555  ,  2.7660267 ,  7.8000865 ,\n",
       "         -4.072202  , -1.5268492 , -1.3989537 ]],\n",
       "\n",
       "       [[-1.623553  ,  6.437792  ,  0.952935  , -2.0380113 ,\n",
       "          1.8217608 , -4.108915  ,  0.6353894 ]],\n",
       "\n",
       "       [[ 0.8852041 , -1.4079202 ,  1.104238  , -0.45888564,\n",
       "         -2.7183044 ,  1.1600124 ,  3.7676055 ]],\n",
       "\n",
       "       [[-1.1969954 , -0.98133934,  3.1646774 ,  0.59719115,\n",
       "         -0.62568814, -0.846568  , -0.16888843]],\n",
       "\n",
       "       [[-0.87773895, -1.2683634 ,  3.1936808 ,  0.14415199,\n",
       "         -1.983511  , -0.04533061,  1.5278798 ]],\n",
       "\n",
       "       [[ 4.840487  , -5.6789594 , -1.6817155 , -1.4062569 ,\n",
       "         -2.3526888 ,  0.28257075,  0.64046234]],\n",
       "\n",
       "       [[-1.2520379 ,  5.9443674 ,  1.410282  , -1.4646583 ,\n",
       "          0.7471036 , -2.8317802 ,  3.0954447 ]],\n",
       "\n",
       "       [[-3.65079   , -1.7132066 ,  6.703961  ,  0.609944  ,\n",
       "         -1.9491858 , -1.3482741 ,  0.09523024]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_model.predict(test_data.paper_id.iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 77.07%\n"
     ]
    }
   ],
   "source": [
    "x_test = test_data.paper_id.to_numpy()\n",
    "_, test_accuracy = gnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = loader.class_values\n",
    "node_features=loader.node_features\n",
    "edges = loader.edges\n",
    "\n",
    "def display_class_probabilities(probabilities):\n",
    "    for instance_idx, probs in enumerate(probabilities):\n",
    "        print(f\"Instance {instance_idx + 1}:\")\n",
    "        for class_idx, prob in enumerate(probs):\n",
    "            print(f\"- {class_values[class_idx]}: {round(prob * 100, 2)}%\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[0] = 2708 is not in [0, 2708)\n\t [[node gnn_model/GatherV2\n (defined at C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_23844/2180048713.py:66)\n]] [Op:__inference_predict_function_15899]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gnn_model/GatherV2:\nIn[0] gnn_model/postprocess/dense_11/Gelu/mul_1 (defined at C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\core\\dense.py:213)\t\nIn[1] IteratorGetNext (defined at C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1610)\t\nIn[2] gnn_model/GatherV2/axis:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 668, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 456, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 445, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 352, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 647, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 335, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_23844/1116990751.py\", line 1, in <module>\n>>>     logits = gnn_model.predict(tf.constant([2708, 2709, 2710]))\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1789, in predict\n>>>     tmp_batch_outputs = self.predict_function(iterator)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step\n>>>     outputs = model.predict_step(data)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n>>>     return self(x, training=False)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_23844/2180048713.py\", line 66, in call\n>>>     node_embeddings = tf.gather(x, input_node_indices)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23844/1116990751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2708\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2709\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2710\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices[:2]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay_class_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[0] = 2708 is not in [0, 2708)\n\t [[node gnn_model/GatherV2\n (defined at C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_23844/2180048713.py:66)\n]] [Op:__inference_predict_function_15899]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gnn_model/GatherV2:\nIn[0] gnn_model/postprocess/dense_11/Gelu/mul_1 (defined at C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\core\\dense.py:213)\t\nIn[1] IteratorGetNext (defined at C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1610)\t\nIn[2] gnn_model/GatherV2/axis:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 668, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 456, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 445, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 352, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 647, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 335, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_23844/1116990751.py\", line 1, in <module>\n>>>     logits = gnn_model.predict(tf.constant([2708, 2709, 2710]))\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1789, in predict\n>>>     tmp_batch_outputs = self.predict_function(iterator)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step\n>>>     outputs = model.predict_step(data)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n>>>     return self(x, training=False)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\USER-PC\\AppData\\Local\\Temp/ipykernel_23844/2180048713.py\", line 66, in call\n>>>     node_embeddings = tf.gather(x, input_node_indices)\n>>> "
     ]
    }
   ],
   "source": [
    "logits = gnn_model.predict(tf.constant([2708, 2709, 2710]))\n",
    "# logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices[:2]))\n",
    "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
    "display_class_probabilities(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5812e37fbc42ae0019c075dcd625ea6adf837b197758e07cfdfe5b415c77a600"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
