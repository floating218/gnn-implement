{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_dataset(self):    \n",
    "        \n",
    "        '''\n",
    "        citations: [target논문인덱스, source논문인덱스]\n",
    "        papers: [논문인덱스, 1424개 단어 포함 여부, 주제(subject)]\n",
    "        train: papers 데이터 중 50% 샘플링\n",
    "        test: papers 데이터 중 50% 샘플링\n",
    "        '''\n",
    "        \n",
    "        zip_file = keras.utils.get_file(\n",
    "            fname=\"cora.tgz\",\n",
    "            origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "            extract=True,\n",
    "        )\n",
    "        data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "        \n",
    "        citations = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.cites\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"target\", \"source\"],\n",
    "        )\n",
    "        \n",
    "        column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "        papers = pd.read_csv(\n",
    "            os.path.join(data_dir, \"cora.content\"), \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=column_names,\n",
    "        )\n",
    "        \n",
    "        class_values = sorted(papers[\"subject\"].unique())\n",
    "        class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "        paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "        papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "        citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "        papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "        \n",
    "        train_data, test_data = [], []\n",
    "\n",
    "        for _, group_data in papers.groupby(\"subject\"):\n",
    "            # Select around 50% of the dataset for training.\n",
    "            random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "            train_data.append(group_data[random_selection])\n",
    "            test_data.append(group_data[~random_selection])\n",
    "\n",
    "        train_data = pd.concat(train_data).sample(frac=1)\n",
    "        test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "        print(\"citations data shape:\", citations.shape)\n",
    "        print(\"papers data shape:\", papers.shape)\n",
    "        print(\"Train data shape:\", train_data.shape)\n",
    "        print(\"Test data shape:\", test_data.shape)\n",
    "        \n",
    "        feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
    "        num_features = len(feature_names)\n",
    "        num_classes = len(class_idx)\n",
    "\n",
    "        # Create train and test features as a numpy array.\n",
    "        x_train = train_data[feature_names].to_numpy()\n",
    "        x_test = test_data[feature_names].to_numpy()\n",
    "        \n",
    "        # Create train and test targets as a numpy array.\n",
    "        y_train = train_data[\"subject\"]\n",
    "        y_test = test_data[\"subject\"]\n",
    "        \n",
    "        return citations, papers, x_train, x_test, y_train, y_test, train_data, test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citations data shape: (5429, 2)\n",
      "papers data shape: (2708, 1435)\n",
      "Train data shape: (1317, 1435)\n",
      "Test data shape: (1391, 1435)\n"
     ]
    }
   ],
   "source": [
    "citations, papers, x_train, x_test, y_train, y_test, train_data, test_data = Loader().load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape: (2, 5429)\n",
      "Nodes shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
    "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "# Create an edge weights array of ones.\n",
    "edge_weights = tf.ones(shape=edges.shape[1])\n",
    "# Create a node features array of shape [num_nodes, num_features].\n",
    "node_features = tf.cast(\n",
    "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    ")\n",
    "# Create graph info tuple with node_features, edges, and edge_weights.\n",
    "graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Nodes shape:\", node_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    '''\n",
    "hidden_units = [32, 32]\n",
    "dropout_rate = 0.2\n",
    "'''\n",
    "\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN output shape: tf.Tensor(\n",
      "[[ 0.04023143 -0.03798582 -0.01061788  0.03870849  0.08173378  0.06705542\n",
      "   0.0176754 ]\n",
      " [-0.0204511  -0.00529465  0.03362372 -0.05983638  0.19713391 -0.0527336\n",
      "  -0.10019694]\n",
      " [ 0.0660347  -0.0315948   0.01978562  0.03459518  0.05062745  0.08475165\n",
      "   0.02827925]], shape=(3, 7), dtype=float32)\n",
      "Model: \"gnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " preprocess (Sequential)     (2708, 32)                52804     \n",
      "                                                                 \n",
      " graph_conv1 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " graph_conv2 (GraphConvLayer  multiple                 5888      \n",
      " )                                                               \n",
      "                                                                 \n",
      " postprocess (Sequential)    (2708, 32)                2368      \n",
      "                                                                 \n",
      " logits (Dense)              multiple                  231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,179\n",
      "Trainable params: 63,481\n",
      "Non-trainable params: 3,698\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "gnn_model = GNNNodeClassifier(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiment(model, x_train, y_train):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 3s 162ms/step - loss: 2.2650 - acc: 0.1537 - val_loss: 1.9017 - val_acc: 0.3131\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.9851 - acc: 0.2377 - val_loss: 1.8790 - val_acc: 0.3131\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 1.9357 - acc: 0.2681 - val_loss: 1.8767 - val_acc: 0.3131\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.8984 - acc: 0.2645 - val_loss: 1.8758 - val_acc: 0.3131\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.8701 - acc: 0.2672 - val_loss: 1.8694 - val_acc: 0.3131\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 1.8651 - acc: 0.2744 - val_loss: 1.8586 - val_acc: 0.3131\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.8614 - acc: 0.2547 - val_loss: 1.8499 - val_acc: 0.3131\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.8309 - acc: 0.2842 - val_loss: 1.8404 - val_acc: 0.3131\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 1.8270 - acc: 0.2913 - val_loss: 1.8310 - val_acc: 0.3131\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 1.8117 - acc: 0.2878 - val_loss: 1.8191 - val_acc: 0.3131\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.7869 - acc: 0.3119 - val_loss: 1.8089 - val_acc: 0.3131\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 1.8069 - acc: 0.3074 - val_loss: 1.7948 - val_acc: 0.3232\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.7450 - acc: 0.3172 - val_loss: 1.7720 - val_acc: 0.3384\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.7561 - acc: 0.3244 - val_loss: 1.7373 - val_acc: 0.4040\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 1.7035 - acc: 0.3530 - val_loss: 1.6961 - val_acc: 0.4040\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 1.6600 - acc: 0.3843 - val_loss: 1.6587 - val_acc: 0.4293\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 1.6285 - acc: 0.3861 - val_loss: 1.6397 - val_acc: 0.4040\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.5518 - acc: 0.4209 - val_loss: 1.6665 - val_acc: 0.3636\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.5048 - acc: 0.4459 - val_loss: 1.6551 - val_acc: 0.3737\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.4457 - acc: 0.4531 - val_loss: 1.6691 - val_acc: 0.4444\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.3908 - acc: 0.4790 - val_loss: 1.7995 - val_acc: 0.4444\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.3754 - acc: 0.4942 - val_loss: 2.0713 - val_acc: 0.3990\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.3089 - acc: 0.5442 - val_loss: 2.0581 - val_acc: 0.4192\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 1.3000 - acc: 0.5523 - val_loss: 1.7961 - val_acc: 0.4899\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.2323 - acc: 0.5621 - val_loss: 1.8855 - val_acc: 0.4899\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.1997 - acc: 0.5639 - val_loss: 2.0312 - val_acc: 0.5000\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 1.1774 - acc: 0.5666 - val_loss: 2.0907 - val_acc: 0.4949\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.1463 - acc: 0.5970 - val_loss: 2.1314 - val_acc: 0.4848\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 1.1206 - acc: 0.6050 - val_loss: 2.1005 - val_acc: 0.5051\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.0967 - acc: 0.5987 - val_loss: 1.9962 - val_acc: 0.5202\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.0709 - acc: 0.6282 - val_loss: 1.9403 - val_acc: 0.4949\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 1.0481 - acc: 0.6211 - val_loss: 1.8554 - val_acc: 0.5051\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 1.0621 - acc: 0.6247 - val_loss: 1.7580 - val_acc: 0.5051\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.9994 - acc: 0.6488 - val_loss: 1.5372 - val_acc: 0.5101\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.9360 - acc: 0.6774 - val_loss: 1.4722 - val_acc: 0.5202\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.9569 - acc: 0.6801 - val_loss: 1.4717 - val_acc: 0.5657\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.9229 - acc: 0.6720 - val_loss: 1.5315 - val_acc: 0.5556\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.8807 - acc: 0.7176 - val_loss: 1.7102 - val_acc: 0.5202\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.8724 - acc: 0.7113 - val_loss: 1.6516 - val_acc: 0.5253\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.8275 - acc: 0.7149 - val_loss: 1.4523 - val_acc: 0.5960\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.8556 - acc: 0.7203 - val_loss: 1.4497 - val_acc: 0.5960\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.8299 - acc: 0.7239 - val_loss: 1.4026 - val_acc: 0.6061\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.7836 - acc: 0.7230 - val_loss: 1.3676 - val_acc: 0.6212\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.7944 - acc: 0.7328 - val_loss: 1.2917 - val_acc: 0.6667\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.7061 - acc: 0.7694 - val_loss: 1.3298 - val_acc: 0.6515\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.7901 - acc: 0.7373 - val_loss: 1.2198 - val_acc: 0.6667\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.7488 - acc: 0.7507 - val_loss: 1.1689 - val_acc: 0.7020\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.7192 - acc: 0.7534 - val_loss: 1.2650 - val_acc: 0.6717\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.7443 - acc: 0.7391 - val_loss: 1.1654 - val_acc: 0.6717\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.6862 - acc: 0.7632 - val_loss: 1.0819 - val_acc: 0.6717\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.6755 - acc: 0.7784 - val_loss: 1.0641 - val_acc: 0.6768\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.6970 - acc: 0.7703 - val_loss: 1.0157 - val_acc: 0.6768\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.7101 - acc: 0.7712 - val_loss: 0.9506 - val_acc: 0.7273\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.6766 - acc: 0.7882 - val_loss: 0.9490 - val_acc: 0.7273\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.6656 - acc: 0.7918 - val_loss: 0.9270 - val_acc: 0.7323\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.6758 - acc: 0.7864 - val_loss: 0.8877 - val_acc: 0.7323\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7052 - acc: 0.7828 - val_loss: 0.8921 - val_acc: 0.7020\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.6168 - acc: 0.8070 - val_loss: 0.9369 - val_acc: 0.7020\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6434 - acc: 0.7989 - val_loss: 0.9861 - val_acc: 0.7121\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6201 - acc: 0.8061 - val_loss: 1.0062 - val_acc: 0.7374\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.6481 - acc: 0.7954 - val_loss: 1.0283 - val_acc: 0.7222\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.6266 - acc: 0.7998 - val_loss: 1.0814 - val_acc: 0.7172\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.6028 - acc: 0.8195 - val_loss: 1.0260 - val_acc: 0.7172\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.6378 - acc: 0.7980 - val_loss: 0.9713 - val_acc: 0.7273\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.6164 - acc: 0.8043 - val_loss: 0.9126 - val_acc: 0.7374\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.5979 - acc: 0.8025 - val_loss: 0.9052 - val_acc: 0.7424\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.5451 - acc: 0.8239 - val_loss: 0.9036 - val_acc: 0.7424\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.6167 - acc: 0.7998 - val_loss: 0.8879 - val_acc: 0.7424\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.5674 - acc: 0.8231 - val_loss: 0.8668 - val_acc: 0.7273\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.5720 - acc: 0.8284 - val_loss: 0.8027 - val_acc: 0.7525\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.5658 - acc: 0.8222 - val_loss: 0.8260 - val_acc: 0.7576\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.5454 - acc: 0.8231 - val_loss: 0.8705 - val_acc: 0.7525\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.5374 - acc: 0.8239 - val_loss: 0.8858 - val_acc: 0.7424\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.5399 - acc: 0.8302 - val_loss: 0.8165 - val_acc: 0.7525\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.5781 - acc: 0.8186 - val_loss: 0.8153 - val_acc: 0.7424\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.5300 - acc: 0.8436 - val_loss: 0.8812 - val_acc: 0.7374\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.5378 - acc: 0.8320 - val_loss: 0.8880 - val_acc: 0.7273\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.5176 - acc: 0.8266 - val_loss: 0.8556 - val_acc: 0.7273\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 0.5245 - acc: 0.8391 - val_loss: 0.7991 - val_acc: 0.7424\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.5267 - acc: 0.8382 - val_loss: 0.7556 - val_acc: 0.7727\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.5469 - acc: 0.8329 - val_loss: 0.8456 - val_acc: 0.7677\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4796 - acc: 0.8561 - val_loss: 0.8556 - val_acc: 0.7525\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4982 - acc: 0.8391 - val_loss: 0.8212 - val_acc: 0.7576\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.4860 - acc: 0.8365 - val_loss: 0.8093 - val_acc: 0.7576\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.5369 - acc: 0.8347 - val_loss: 0.8001 - val_acc: 0.7727\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4727 - acc: 0.8472 - val_loss: 0.7917 - val_acc: 0.7626\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4788 - acc: 0.8508 - val_loss: 0.7918 - val_acc: 0.7778\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4941 - acc: 0.8436 - val_loss: 0.8044 - val_acc: 0.7677\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4725 - acc: 0.8490 - val_loss: 0.8204 - val_acc: 0.7525\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4958 - acc: 0.8490 - val_loss: 0.7569 - val_acc: 0.7576\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4676 - acc: 0.8525 - val_loss: 0.7472 - val_acc: 0.7525\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4641 - acc: 0.8490 - val_loss: 0.7483 - val_acc: 0.7576\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4900 - acc: 0.8445 - val_loss: 0.7701 - val_acc: 0.7727\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.4408 - acc: 0.8606 - val_loss: 0.8110 - val_acc: 0.7727\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4948 - acc: 0.8418 - val_loss: 0.8006 - val_acc: 0.7727\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.4173 - acc: 0.8695 - val_loss: 0.8069 - val_acc: 0.7677\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4377 - acc: 0.8606 - val_loss: 0.7861 - val_acc: 0.7626\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4193 - acc: 0.8668 - val_loss: 0.7603 - val_acc: 0.7778\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.4777 - acc: 0.8597 - val_loss: 0.7544 - val_acc: 0.7828\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.4636 - acc: 0.8588 - val_loss: 0.7441 - val_acc: 0.7727\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4862 - acc: 0.8517 - val_loss: 0.7695 - val_acc: 0.7626\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4703 - acc: 0.8454 - val_loss: 0.7519 - val_acc: 0.7828\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4694 - acc: 0.8588 - val_loss: 0.7278 - val_acc: 0.7929\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.4226 - acc: 0.8695 - val_loss: 0.7184 - val_acc: 0.7929\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.4474 - acc: 0.8677 - val_loss: 0.7332 - val_acc: 0.7828\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3799 - acc: 0.8820 - val_loss: 0.7785 - val_acc: 0.7778\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4707 - acc: 0.8588 - val_loss: 0.7540 - val_acc: 0.7727\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4245 - acc: 0.8642 - val_loss: 0.7484 - val_acc: 0.7879\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4254 - acc: 0.8677 - val_loss: 0.7430 - val_acc: 0.7828\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4421 - acc: 0.8615 - val_loss: 0.7407 - val_acc: 0.7828\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4063 - acc: 0.8695 - val_loss: 0.7411 - val_acc: 0.7929\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4138 - acc: 0.8660 - val_loss: 0.7355 - val_acc: 0.7828\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3979 - acc: 0.8794 - val_loss: 0.7230 - val_acc: 0.7778\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.3993 - acc: 0.8767 - val_loss: 0.7330 - val_acc: 0.7828\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4375 - acc: 0.8677 - val_loss: 0.7403 - val_acc: 0.7828\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4292 - acc: 0.8651 - val_loss: 0.7385 - val_acc: 0.7626\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3770 - acc: 0.8713 - val_loss: 0.7644 - val_acc: 0.7828\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4803 - acc: 0.8499 - val_loss: 0.7769 - val_acc: 0.7929\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4189 - acc: 0.8633 - val_loss: 0.7395 - val_acc: 0.7828\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3863 - acc: 0.8820 - val_loss: 0.7375 - val_acc: 0.7929\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.4344 - acc: 0.8624 - val_loss: 0.7616 - val_acc: 0.7929\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.4608 - acc: 0.8615 - val_loss: 0.7636 - val_acc: 0.7778\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3839 - acc: 0.8803 - val_loss: 0.7862 - val_acc: 0.7677\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4114 - acc: 0.8829 - val_loss: 0.7428 - val_acc: 0.7677\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4178 - acc: 0.8597 - val_loss: 0.7036 - val_acc: 0.7778\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3342 - acc: 0.8937 - val_loss: 0.7192 - val_acc: 0.7727\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4160 - acc: 0.8722 - val_loss: 0.7704 - val_acc: 0.7778\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4285 - acc: 0.8758 - val_loss: 0.7436 - val_acc: 0.7727\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3943 - acc: 0.8829 - val_loss: 0.6917 - val_acc: 0.7929\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.3617 - acc: 0.8820 - val_loss: 0.7057 - val_acc: 0.7828\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3481 - acc: 0.8847 - val_loss: 0.7126 - val_acc: 0.7929\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.3954 - acc: 0.8803 - val_loss: 0.7315 - val_acc: 0.7929\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.4459 - acc: 0.8722 - val_loss: 0.7433 - val_acc: 0.7929\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3949 - acc: 0.8767 - val_loss: 0.7468 - val_acc: 0.7828\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.4166 - acc: 0.8803 - val_loss: 0.7518 - val_acc: 0.7879\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3811 - acc: 0.8829 - val_loss: 0.7322 - val_acc: 0.7980\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3836 - acc: 0.8803 - val_loss: 0.7504 - val_acc: 0.7879\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3354 - acc: 0.8990 - val_loss: 0.7421 - val_acc: 0.7879\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3612 - acc: 0.8829 - val_loss: 0.7317 - val_acc: 0.7727\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3713 - acc: 0.8883 - val_loss: 0.7439 - val_acc: 0.7929\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3667 - acc: 0.8883 - val_loss: 0.7663 - val_acc: 0.8030\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3506 - acc: 0.8937 - val_loss: 0.7709 - val_acc: 0.7778\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3707 - acc: 0.8847 - val_loss: 0.7613 - val_acc: 0.7778\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.4000 - acc: 0.8785 - val_loss: 0.7134 - val_acc: 0.7929\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3785 - acc: 0.8874 - val_loss: 0.6638 - val_acc: 0.7980\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3589 - acc: 0.8838 - val_loss: 0.6655 - val_acc: 0.7929\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3760 - acc: 0.8874 - val_loss: 0.7437 - val_acc: 0.8131\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.3860 - acc: 0.8776 - val_loss: 0.7967 - val_acc: 0.8131\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.4175 - acc: 0.8767 - val_loss: 0.7617 - val_acc: 0.8081\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 0.3444 - acc: 0.8892 - val_loss: 0.7236 - val_acc: 0.7980\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3758 - acc: 0.8794 - val_loss: 0.6971 - val_acc: 0.7929\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3901 - acc: 0.8811 - val_loss: 0.6941 - val_acc: 0.7879\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3388 - acc: 0.8945 - val_loss: 0.7318 - val_acc: 0.7778\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.4327 - acc: 0.8686 - val_loss: 0.7548 - val_acc: 0.7727\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3422 - acc: 0.8892 - val_loss: 0.7253 - val_acc: 0.7828\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3620 - acc: 0.8945 - val_loss: 0.6887 - val_acc: 0.7980\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3666 - acc: 0.8856 - val_loss: 0.6834 - val_acc: 0.8030\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3789 - acc: 0.8785 - val_loss: 0.7566 - val_acc: 0.7828\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3830 - acc: 0.8811 - val_loss: 0.7624 - val_acc: 0.7778\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3730 - acc: 0.8856 - val_loss: 0.7217 - val_acc: 0.7879\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3523 - acc: 0.8883 - val_loss: 0.6824 - val_acc: 0.7929\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3725 - acc: 0.8758 - val_loss: 0.7113 - val_acc: 0.7980\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3782 - acc: 0.8883 - val_loss: 0.7070 - val_acc: 0.7828\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3952 - acc: 0.8811 - val_loss: 0.6938 - val_acc: 0.7828\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3482 - acc: 0.8937 - val_loss: 0.6945 - val_acc: 0.8030\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.4008 - acc: 0.8811 - val_loss: 0.7273 - val_acc: 0.7980\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.4173 - acc: 0.8865 - val_loss: 0.7760 - val_acc: 0.7879\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.3665 - acc: 0.8883 - val_loss: 0.7768 - val_acc: 0.7879\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3402 - acc: 0.8972 - val_loss: 0.7599 - val_acc: 0.7778\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3498 - acc: 0.8856 - val_loss: 0.7397 - val_acc: 0.7879\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3025 - acc: 0.9071 - val_loss: 0.7542 - val_acc: 0.7828\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3553 - acc: 0.8847 - val_loss: 0.7590 - val_acc: 0.8030\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.3540 - acc: 0.9017 - val_loss: 0.7609 - val_acc: 0.8131\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3342 - acc: 0.9062 - val_loss: 0.7616 - val_acc: 0.7980\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3656 - acc: 0.8829 - val_loss: 0.7516 - val_acc: 0.7929\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3331 - acc: 0.8963 - val_loss: 0.7853 - val_acc: 0.8030\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3440 - acc: 0.9026 - val_loss: 0.8338 - val_acc: 0.7929\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3392 - acc: 0.9106 - val_loss: 0.8221 - val_acc: 0.7929\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3277 - acc: 0.9026 - val_loss: 0.7873 - val_acc: 0.8030\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3535 - acc: 0.8937 - val_loss: 0.7709 - val_acc: 0.7929\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3198 - acc: 0.8928 - val_loss: 0.7916 - val_acc: 0.7879\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3324 - acc: 0.8954 - val_loss: 0.8279 - val_acc: 0.7879\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.3279 - acc: 0.9026 - val_loss: 0.8375 - val_acc: 0.7828\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3283 - acc: 0.9044 - val_loss: 0.7968 - val_acc: 0.7929\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3418 - acc: 0.9008 - val_loss: 0.7848 - val_acc: 0.7929\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3843 - acc: 0.8758 - val_loss: 0.7868 - val_acc: 0.7929\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.3771 - acc: 0.8820 - val_loss: 0.7721 - val_acc: 0.7929\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3643 - acc: 0.8803 - val_loss: 0.7781 - val_acc: 0.7879\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.3853 - acc: 0.8910 - val_loss: 0.7466 - val_acc: 0.7980\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.3186 - acc: 0.9008 - val_loss: 0.7241 - val_acc: 0.7929\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.3341 - acc: 0.9053 - val_loss: 0.7329 - val_acc: 0.7879\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3531 - acc: 0.8990 - val_loss: 0.7187 - val_acc: 0.8081\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.3293 - acc: 0.9097 - val_loss: 0.7222 - val_acc: 0.8030\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.3781 - acc: 0.8865 - val_loss: 0.7508 - val_acc: 0.7828\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.3388 - acc: 0.8963 - val_loss: 0.7787 - val_acc: 0.7828\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.3494 - acc: 0.8981 - val_loss: 0.7357 - val_acc: 0.8030\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.3211 - acc: 0.9026 - val_loss: 0.7162 - val_acc: 0.7980\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data.paper_id.to_numpy()\n",
    "history = run_experiment(gnn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5812e37fbc42ae0019c075dcd625ea6adf837b197758e07cfdfe5b415c77a600"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
